//! x86_64 instruction lifter.
//!
//! This module lifts x86_64 machine code to SMIR. Unlike AArch64 which has a clean
//! decoder, x86 decoding is interleaved with lifting due to variable-length encoding.

use std::collections::HashSet;

use crate::smir::flags::FlagUpdate;
use crate::smir::ir::{
    CallTarget, CallingConv, FunctionAttrs, SmirBlock, SmirFunction, Terminator, TrapKind,
};
use crate::smir::lift::{
    ControlFlow, LiftContext, LiftError, LiftResult, MemoryReader, SmirLifter,
};
use crate::smir::memory::MemoryError;
use crate::smir::ops::{OpKind, SmirOp, X86AluEncoding, X86OpHint, X86SsePrefix, X86VecMap};
use crate::smir::types::*;

// ============================================================================
// x86_64 Lifter
// ============================================================================

/// x86_64 instruction lifter
pub struct X86_64Lifter {
    /// Whether to use strict mode (fail on unsupported instructions)
    strict: bool,
}

impl Default for X86_64Lifter {
    fn default() -> Self {
        Self::new()
    }
}

impl X86_64Lifter {
    /// Create a new x86_64 lifter
    pub fn new() -> Self {
        X86_64Lifter { strict: false }
    }

    /// Create a lifter in strict mode
    pub fn strict() -> Self {
        X86_64Lifter { strict: true }
    }
}

// ============================================================================
// Prefix Decoding
// ============================================================================

/// Lookup table for prefix detection
static PREFIX_LUT: [u8; 256] = {
    let mut lut = [0u8; 256];
    // Segment overrides
    lut[0x26] = 1; // ES
    lut[0x2E] = 1; // CS
    lut[0x36] = 1; // SS
    lut[0x3E] = 1; // DS
    lut[0x64] = 1; // FS
    lut[0x65] = 1; // GS
                   // Operand/address size
    lut[0x66] = 1;
    lut[0x67] = 1;
    // LOCK, REP
    lut[0xF0] = 1;
    lut[0xF2] = 1;
    lut[0xF3] = 1;
    // REX (0x40-0x4F)
    let mut i = 0x40u8;
    while i <= 0x4F {
        lut[i as usize] = 1;
        i += 1;
    }
    lut
};

/// Decoded x86 instruction prefix state
#[derive(Clone, Debug, Default)]
pub struct X86Prefix {
    /// REX prefix if present
    pub rex: Option<u8>,
    /// Operand size override (0x66)
    pub operand_size_override: bool,
    /// Address size override (0x67)
    pub address_size_override: bool,
    /// REP/REPNE prefix
    pub rep_prefix: Option<u8>,
    /// Segment override
    pub segment_override: Option<u8>,
    /// LOCK prefix
    pub lock: bool,
    /// Cursor position after prefixes
    pub cursor: usize,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq)]
enum VecEncodingKind {
    Vex,
    Evex,
}

#[derive(Clone, Copy, Debug)]
struct VecPrefix {
    encoding: VecEncodingKind,
    map: X86VecMap,
    pp: X86SsePrefix,
    width: VecWidth,
    w: bool,
    vvvv: u8,
    rex: Option<u8>,
    bytes: usize,
}

impl X86Prefix {
    /// Get REX.W flag
    #[inline]
    pub fn rex_w(&self) -> bool {
        self.rex.map_or(false, |r| r & 0x08 != 0)
    }

    /// Get REX.R flag (extends ModR/M reg field)
    #[inline]
    pub fn rex_r(&self) -> u8 {
        self.rex.map_or(0, |r| (r & 0x04) << 1)
    }

    /// Get REX.X flag (extends SIB index field)
    #[inline]
    pub fn rex_x(&self) -> u8 {
        self.rex.map_or(0, |r| (r & 0x02) << 2)
    }

    /// Get REX.B flag (extends ModR/M r/m or opcode reg)
    #[inline]
    pub fn rex_b(&self) -> u8 {
        self.rex.map_or(0, |r| (r & 0x01) << 3)
    }

    /// Check if any REX prefix is present
    #[inline]
    pub fn has_rex(&self) -> bool {
        self.rex.is_some()
    }

    /// Compute operand size for 64-bit mode
    #[inline]
    pub fn op_size(&self) -> u8 {
        if self.rex_w() {
            8
        } else if self.operand_size_override {
            2
        } else {
            4
        }
    }

    /// Compute operand width for SMIR
    #[inline]
    pub fn op_width(&self) -> OpWidth {
        match self.op_size() {
            1 => OpWidth::W8,
            2 => OpWidth::W16,
            4 => OpWidth::W32,
            8 => OpWidth::W64,
            _ => OpWidth::W32,
        }
    }
}

/// Decode instruction prefixes
fn decode_prefixes(bytes: &[u8]) -> Result<X86Prefix, LiftError> {
    if bytes.is_empty() {
        return Err(LiftError::Incomplete {
            addr: 0,
            have: 0,
            need: 1,
        });
    }

    let mut prefix = X86Prefix::default();
    let mut cursor = 0;

    while cursor < bytes.len() {
        let b = bytes[cursor];
        if PREFIX_LUT[b as usize] == 0 {
            break;
        }

        match b {
            0x66 => prefix.operand_size_override = true,
            0x67 => prefix.address_size_override = true,
            0x40..=0x4F => prefix.rex = Some(b),
            0xF0 => prefix.lock = true,
            0xF2 | 0xF3 => prefix.rep_prefix = Some(b),
            0x26 | 0x2E | 0x36 | 0x3E | 0x64 | 0x65 => {
                prefix.segment_override = Some(b);
            }
            _ => break,
        }
        cursor += 1;
    }

    prefix.cursor = cursor;
    Ok(prefix)
}

fn vex_pp_to_prefix(pp: u8) -> X86SsePrefix {
    match pp & 0x3 {
        0 => X86SsePrefix::None,
        1 => X86SsePrefix::OpSize,
        2 => X86SsePrefix::Rep,
        _ => X86SsePrefix::Repne,
    }
}

fn vec_map_from_bits(map: u8) -> Option<X86VecMap> {
    match map {
        0x01 => Some(X86VecMap::Map0F),
        0x02 => Some(X86VecMap::Map0F38),
        0x03 => Some(X86VecMap::Map0F3A),
        _ => None,
    }
}

fn build_rex(r: u8, x: u8, b: u8, w: bool) -> Option<u8> {
    let mut rex = 0x40;
    if w {
        rex |= 0x08;
    }
    if r != 0 {
        rex |= 0x04;
    }
    if x != 0 {
        rex |= 0x02;
    }
    if b != 0 {
        rex |= 0x01;
    }
    if rex == 0x40 {
        None
    } else {
        Some(rex)
    }
}

fn decode_vex_prefix(bytes: &[u8], addr: u64) -> Result<VecPrefix, LiftError> {
    if bytes.is_empty() {
        return Err(LiftError::Incomplete {
            addr,
            have: 0,
            need: 1,
        });
    }

    match bytes[0] {
        0xC5 => {
            if bytes.len() < 2 {
                return Err(LiftError::Incomplete {
                    addr,
                    have: bytes.len(),
                    need: 2,
                });
            }
            let b1 = bytes[1];
            let r = ((b1 >> 7) & 1) ^ 1;
            let vvvv = (!b1 >> 3) & 0x0F;
            let l = (b1 >> 2) & 1;
            let pp = vex_pp_to_prefix(b1 & 0x3);

            Ok(VecPrefix {
                encoding: VecEncodingKind::Vex,
                map: X86VecMap::Map0F,
                pp,
                width: if l == 1 {
                    VecWidth::V256
                } else {
                    VecWidth::V128
                },
                w: false,
                vvvv,
                rex: build_rex(r, 0, 0, false),
                bytes: 2,
            })
        }
        0xC4 => {
            if bytes.len() < 3 {
                return Err(LiftError::Incomplete {
                    addr,
                    have: bytes.len(),
                    need: 3,
                });
            }
            let b1 = bytes[1];
            let b2 = bytes[2];
            let r = ((b1 >> 7) & 1) ^ 1;
            let x = ((b1 >> 6) & 1) ^ 1;
            let b = ((b1 >> 5) & 1) ^ 1;
            let map = vec_map_from_bits(b1 & 0x1F).ok_or_else(|| LiftError::Unsupported {
                addr,
                mnemonic: format!("VEX map 0x{:02X}", b1 & 0x1F),
            })?;
            let w = (b2 >> 7) & 1 != 0;
            let vvvv = (!b2 >> 3) & 0x0F;
            let l = (b2 >> 2) & 1;
            let pp = vex_pp_to_prefix(b2 & 0x3);

            Ok(VecPrefix {
                encoding: VecEncodingKind::Vex,
                map,
                pp,
                width: if l == 1 {
                    VecWidth::V256
                } else {
                    VecWidth::V128
                },
                w,
                vvvv,
                rex: build_rex(r, x, b, w),
                bytes: 3,
            })
        }
        _ => Err(LiftError::Unsupported {
            addr,
            mnemonic: "VEX prefix".to_string(),
        }),
    }
}

fn decode_evex_prefix(bytes: &[u8], addr: u64) -> Result<VecPrefix, LiftError> {
    if bytes.len() < 4 {
        return Err(LiftError::Incomplete {
            addr,
            have: bytes.len(),
            need: 4,
        });
    }

    let b1 = bytes[1];
    let b2 = bytes[2];
    let b3 = bytes[3];

    let r = ((b1 >> 4) & 1) ^ 1;
    let x = ((b1 >> 6) & 1) ^ 1;
    let b = ((b1 >> 5) & 1) ^ 1;
    let map = vec_map_from_bits(b1 & 0x0F).ok_or_else(|| LiftError::Unsupported {
        addr,
        mnemonic: format!("EVEX map 0x{:02X}", b1 & 0x0F),
    })?;

    let w = (b2 >> 7) & 1 != 0;
    let vvvv = (!b2 >> 3) & 0x0F;
    let pp = vex_pp_to_prefix(b2 & 0x3);

    let l_bits = (b3 >> 5) & 0x3;
    let width = match l_bits {
        0 => VecWidth::V128,
        1 => VecWidth::V256,
        2 => VecWidth::V512,
        _ => VecWidth::V512,
    };

    Ok(VecPrefix {
        encoding: VecEncodingKind::Evex,
        map,
        pp,
        width,
        w,
        vvvv,
        rex: build_rex(r, x, b, w),
        bytes: 4,
    })
}

// ============================================================================
// ModR/M and SIB Decoding
// ============================================================================

/// Decoded ModR/M result
#[derive(Clone, Debug)]
pub struct ModRm {
    /// ModR/M byte value
    pub byte: u8,
    /// mod field (0-3)
    pub mod_bits: u8,
    /// reg field with REX.R (0-15)
    pub reg: u8,
    /// r/m field with REX.B (0-15)
    pub rm: u8,
    /// Is this a memory operand (mod != 3)?
    pub is_memory: bool,
    /// Decoded memory address (if is_memory)
    pub addr: Option<X86Address>,
    /// Total bytes consumed (including SIB and displacement)
    pub bytes_consumed: usize,
}

/// x86 memory address representation for lifting
#[derive(Clone, Debug)]
pub struct X86Address {
    /// Base register (None for absolute addresses)
    pub base: Option<u8>,
    /// Index register (None if no index)
    pub index: Option<u8>,
    /// Scale (1, 2, 4, or 8)
    pub scale: u8,
    /// Displacement
    pub disp: i64,
    /// RIP-relative addressing?
    pub rip_relative: bool,
    /// Displacement size hint
    pub disp_size: DispSize,
}

/// Decode ModR/M byte and any following SIB/displacement
fn decode_modrm(bytes: &[u8], prefix: &X86Prefix, addr: u64) -> Result<ModRm, LiftError> {
    if bytes.is_empty() {
        return Err(LiftError::Incomplete {
            addr,
            have: 0,
            need: 1,
        });
    }

    let modrm = bytes[0];
    let mod_bits = modrm >> 6;
    let reg_field = (modrm >> 3) & 0x07;
    let rm_field = modrm & 0x07;

    let reg = reg_field | prefix.rex_r();
    let rm = rm_field | prefix.rex_b();

    if mod_bits == 3 {
        // Register operand
        return Ok(ModRm {
            byte: modrm,
            mod_bits,
            reg,
            rm,
            is_memory: false,
            addr: None,
            bytes_consumed: 1,
        });
    }

    // Memory operand - decode SIB and displacement
    let mut consumed = 1;
    let mut x86_addr = X86Address {
        base: None,
        index: None,
        scale: 1,
        disp: 0,
        rip_relative: false,
        disp_size: DispSize::Auto,
    };

    if rm_field == 4 {
        // SIB byte follows
        if bytes.len() < 2 {
            return Err(LiftError::Incomplete {
                addr,
                have: bytes.len(),
                need: 2,
            });
        }
        let sib = bytes[1];
        consumed += 1;

        let scale = 1u8 << (sib >> 6);
        let index_field = (sib >> 3) & 0x07;
        let base_field = sib & 0x07;

        let index = index_field | prefix.rex_x();
        let base = base_field | prefix.rex_b();

        x86_addr.scale = scale;

        // Index = 4 means no index
        if index != 4 {
            x86_addr.index = Some(index);
        }

        // Handle base
        if base_field == 5 && mod_bits == 0 {
            // No base, disp32 follows
            if bytes.len() < consumed + 4 {
                return Err(LiftError::Incomplete {
                    addr,
                    have: bytes.len(),
                    need: consumed + 4,
                });
            }
            let disp = i32::from_le_bytes([
                bytes[consumed],
                bytes[consumed + 1],
                bytes[consumed + 2],
                bytes[consumed + 3],
            ]) as i64;
            consumed += 4;
            x86_addr.disp = disp;
            x86_addr.disp_size = DispSize::Disp32;
        } else {
            x86_addr.base = Some(base);
        }
    } else if rm_field == 5 && mod_bits == 0 {
        // RIP-relative addressing in 64-bit mode
        if bytes.len() < 5 {
            return Err(LiftError::Incomplete {
                addr,
                have: bytes.len(),
                need: 5,
            });
        }
        let disp = i32::from_le_bytes([bytes[1], bytes[2], bytes[3], bytes[4]]) as i64;
        consumed += 4;
        x86_addr.disp = disp;
        x86_addr.rip_relative = true;
        x86_addr.disp_size = DispSize::Disp32;
    } else {
        // Regular register indirect
        x86_addr.base = Some(rm);
    }

    // Handle displacement for mod=1 (disp8) and mod=2 (disp32)
    match mod_bits {
        1 => {
            if bytes.len() < consumed + 1 {
                return Err(LiftError::Incomplete {
                    addr,
                    have: bytes.len(),
                    need: consumed + 1,
                });
            }
            x86_addr.disp = bytes[consumed] as i8 as i64;
            consumed += 1;
            x86_addr.disp_size = DispSize::Disp8;
        }
        2 => {
            if bytes.len() < consumed + 4 {
                return Err(LiftError::Incomplete {
                    addr,
                    have: bytes.len(),
                    need: consumed + 4,
                });
            }
            x86_addr.disp = i32::from_le_bytes([
                bytes[consumed],
                bytes[consumed + 1],
                bytes[consumed + 2],
                bytes[consumed + 3],
            ]) as i64;
            consumed += 4;
            x86_addr.disp_size = DispSize::Disp32;
        }
        _ => {}
    }

    Ok(ModRm {
        byte: modrm,
        mod_bits,
        reg,
        rm,
        is_memory: true,
        addr: Some(x86_addr),
        bytes_consumed: consumed,
    })
}

// ============================================================================
// Register Helpers
// ============================================================================

impl X86_64Lifter {
    /// Convert x86 register number to VReg
    fn x86_gpr(&self, reg: u8) -> VReg {
        VReg::Arch(ArchReg::X86(X86Reg::gpr(reg)))
    }

    /// Get x86 register by number
    fn gpr(&self, reg: u8) -> VReg {
        self.x86_gpr(reg & 0x0F)
    }

    fn xmm(&self, reg: u8) -> VReg {
        VReg::Arch(ArchReg::X86(X86Reg::Xmm(reg)))
    }

    fn ymm(&self, reg: u8) -> VReg {
        VReg::Arch(ArchReg::X86(X86Reg::Ymm(reg)))
    }

    fn zmm(&self, reg: u8) -> VReg {
        VReg::Arch(ArchReg::X86(X86Reg::Zmm(reg)))
    }

    fn vec_reg(&self, reg: u8, width: VecWidth) -> VReg {
        match width {
            VecWidth::V128 => self.xmm(reg),
            VecWidth::V256 => self.ymm(reg),
            VecWidth::V512 => self.zmm(reg),
            VecWidth::V64 => self.xmm(reg),
        }
    }

    /// Get RSP register
    fn rsp(&self) -> VReg {
        VReg::Arch(ArchReg::X86(X86Reg::Rsp))
    }

    /// Convert op_size to OpWidth
    fn size_to_width(&self, size: u8) -> OpWidth {
        match size {
            1 => OpWidth::W8,
            2 => OpWidth::W16,
            4 => OpWidth::W32,
            8 => OpWidth::W64,
            _ => OpWidth::W32,
        }
    }

    /// Convert op_size to MemWidth
    fn size_to_memwidth(&self, size: u8) -> MemWidth {
        match size {
            1 => MemWidth::B1,
            2 => MemWidth::B2,
            4 => MemWidth::B4,
            8 => MemWidth::B8,
            _ => MemWidth::B4,
        }
    }

    /// Convert x86 address to SMIR Address, optionally generating pre-ops
    fn x86_addr_to_smir(
        &self,
        x86_addr: &X86Address,
        next_rip: u64,
        ctx: &mut LiftContext,
    ) -> (Address, Vec<SmirOp>) {
        let mut pre_ops = Vec::new();
        let pc = ctx.guest_pc;
        let disp_i32 = |disp: i64| -> Option<i32> {
            if disp >= i32::MIN as i64 && disp <= i32::MAX as i64 {
                Some(disp as i32)
            } else {
                None
            }
        };

        if x86_addr.rip_relative {
            return (
                Address::PcRel {
                    offset: x86_addr.disp,
                    disp_size: x86_addr.disp_size,
                    base: Some(next_rip),
                },
                pre_ops,
            );
        }

        match (x86_addr.base, x86_addr.index) {
            (None, None) => {
                // Absolute address
                (Address::Absolute(x86_addr.disp as u64), pre_ops)
            }
            (Some(base), None) => {
                if x86_addr.disp == 0 && x86_addr.disp_size == DispSize::Auto {
                    (Address::Direct(self.gpr(base)), pre_ops)
                } else {
                    (
                        Address::BaseOffset {
                            base: self.gpr(base),
                            offset: x86_addr.disp,
                            disp_size: x86_addr.disp_size,
                        },
                        pre_ops,
                    )
                }
            }
            (None, Some(index)) => {
                if let Some(disp) = disp_i32(x86_addr.disp) {
                    (
                        Address::BaseIndexScale {
                            base: None,
                            index: self.gpr(index),
                            scale: x86_addr.scale,
                            disp,
                            disp_size: x86_addr.disp_size,
                        },
                        pre_ops,
                    )
                } else {
                    // Fallback to computed address
                    let tmp = ctx.alloc_vreg();
                    if x86_addr.scale > 1 {
                        pre_ops.push(SmirOp::new(
                            OpId(0),
                            pc,
                            OpKind::Shl {
                                dst: tmp,
                                src: self.gpr(index),
                                amount: SrcOperand::Imm(x86_addr.scale.trailing_zeros() as i64),
                                width: OpWidth::W64,
                                flags: FlagUpdate::None,
                            },
                        ));
                        if x86_addr.disp != 0 {
                            let tmp2 = ctx.alloc_vreg();
                            pre_ops.push(SmirOp::new(
                                OpId(1),
                                pc,
                                OpKind::Add {
                                    dst: tmp2,
                                    src1: tmp,
                                    src2: SrcOperand::Imm(x86_addr.disp),
                                    width: OpWidth::W64,
                                    flags: FlagUpdate::None,
                                },
                            ));
                            (Address::Direct(tmp2), pre_ops)
                        } else {
                            (Address::Direct(tmp), pre_ops)
                        }
                    } else if x86_addr.disp != 0 {
                        pre_ops.push(SmirOp::new(
                            OpId(0),
                            pc,
                            OpKind::Add {
                                dst: tmp,
                                src1: self.gpr(index),
                                src2: SrcOperand::Imm(x86_addr.disp),
                                width: OpWidth::W64,
                                flags: FlagUpdate::None,
                            },
                        ));
                        (Address::Direct(tmp), pre_ops)
                    } else {
                        (Address::Direct(self.gpr(index)), pre_ops)
                    }
                }
            }
            (Some(base), Some(index)) => {
                if let Some(disp) = disp_i32(x86_addr.disp) {
                    (
                        Address::BaseIndexScale {
                            base: Some(self.gpr(base)),
                            index: self.gpr(index),
                            scale: x86_addr.scale,
                            disp,
                            disp_size: x86_addr.disp_size,
                        },
                        pre_ops,
                    )
                } else {
                    // Fallback to computed address
                    let tmp_idx = ctx.alloc_vreg();
                    let tmp_sum = ctx.alloc_vreg();

                    // Scale the index
                    if x86_addr.scale > 1 {
                        pre_ops.push(SmirOp::new(
                            OpId(0),
                            pc,
                            OpKind::Shl {
                                dst: tmp_idx,
                                src: self.gpr(index),
                                amount: SrcOperand::Imm(x86_addr.scale.trailing_zeros() as i64),
                                width: OpWidth::W64,
                                flags: FlagUpdate::None,
                            },
                        ));
                        pre_ops.push(SmirOp::new(
                            OpId(1),
                            pc,
                            OpKind::Add {
                                dst: tmp_sum,
                                src1: self.gpr(base),
                                src2: SrcOperand::Reg(tmp_idx),
                                width: OpWidth::W64,
                                flags: FlagUpdate::None,
                            },
                        ));
                    } else {
                        pre_ops.push(SmirOp::new(
                            OpId(0),
                            pc,
                            OpKind::Add {
                                dst: tmp_sum,
                                src1: self.gpr(base),
                                src2: SrcOperand::Reg(self.gpr(index)),
                                width: OpWidth::W64,
                                flags: FlagUpdate::None,
                            },
                        ));
                    }

                    if x86_addr.disp != 0 {
                        (
                            Address::BaseOffset {
                                base: tmp_sum,
                                offset: x86_addr.disp,
                                disp_size: x86_addr.disp_size,
                            },
                            pre_ops,
                        )
                    } else {
                        (Address::Direct(tmp_sum), pre_ops)
                    }
                }
            }
        }
    }

    /// Map x86 condition code (0-15) to SMIR Condition
    fn x86_cond(&self, cc: u8) -> Condition {
        match cc & 0x0F {
            0x0 => Condition::Overflow,   // O
            0x1 => Condition::NoOverflow, // NO
            0x2 => Condition::Ult,        // B/C/NAE
            0x3 => Condition::Uge,        // AE/NB/NC
            0x4 => Condition::Eq,         // E/Z
            0x5 => Condition::Ne,         // NE/NZ
            0x6 => Condition::Ule,        // BE/NA
            0x7 => Condition::Ugt,        // A/NBE
            0x8 => Condition::Negative,   // S
            0x9 => Condition::Positive,   // NS
            0xA => Condition::Parity,     // P/PE
            0xB => Condition::NoParity,   // NP/PO
            0xC => Condition::Slt,        // L/NGE
            0xD => Condition::Sge,        // GE/NL
            0xE => Condition::Sle,        // LE/NG
            0xF => Condition::Sgt,        // G/NLE
            _ => Condition::Always,
        }
    }
}

// ============================================================================
// Instruction Lifting
// ============================================================================

impl X86_64Lifter {
    /// Lift arithmetic instruction (ADD, SUB, ADC, SBC, CMP)
    fn lift_arith(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        // Determine operation type from opcode
        let (is_8bit, dir_rm_reg) = match opcode & 0x07 {
            0 => (true, true),   // rm8, r8
            1 => (false, true),  // rm, r
            2 => (true, false),  // r8, rm8
            3 => (false, false), // r, rm
            4 => (true, true),   // AL, imm8 (handled separately)
            5 => (false, true),  // rAX, imm (handled separately)
            _ => {
                return Err(LiftError::InvalidEncoding {
                    addr: pc,
                    bytes: bytes.to_vec(),
                })
            }
        };

        let op_size = if is_8bit { 1 } else { prefix.op_size() };
        let width = self.size_to_width(op_size);

        if (opcode & 0x07) == 4 || (opcode & 0x07) == 5 {
            let imm_size = if is_8bit {
                1
            } else if op_size == 2 {
                2
            } else {
                4
            };
            if bytes.len() < imm_size {
                return Err(LiftError::Incomplete {
                    addr: pc,
                    have: bytes.len(),
                    need: imm_size,
                });
            }

            let imm = match imm_size {
                1 => bytes[0] as i8 as i64,
                2 => i16::from_le_bytes([bytes[0], bytes[1]]) as i64,
                _ => i32::from_le_bytes([bytes[0], bytes[1], bytes[2], bytes[3]]) as i64,
            };

            let dst = self.gpr(0);
            let hint = X86OpHint::AluEncoding(X86AluEncoding::AccImm);
            let op_kind = match (opcode >> 3) & 0x07 {
                0 => OpKind::Add {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
                1 => OpKind::Or {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
                2 => OpKind::Adc {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
                3 => OpKind::Sbb {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
                4 => OpKind::And {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
                5 => OpKind::Sub {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
                6 => OpKind::Xor {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
                7 => {
                    let op = SmirOp::with_hint(
                        OpId(0),
                        pc,
                        OpKind::Cmp {
                            src1: dst,
                            src2: SrcOperand::Imm(imm),
                            width,
                        },
                        hint,
                    );
                    return Ok(LiftResult::fallthrough(vec![op], prefix.cursor + imm_size));
                }
                _ => unreachable!(),
            };

            let op = SmirOp::with_hint(OpId(0), pc, op_kind, hint);
            return Ok(LiftResult::fallthrough(vec![op], prefix.cursor + imm_size));
        }

        // Decode ModR/M
        let modrm = decode_modrm(bytes, prefix, pc)?;
        let mut ops = Vec::new();
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;

        // Get source and destination
        let (dst, src) = if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            if dir_rm_reg {
                // rm is destination, reg is source
                let tmp = ctx.alloc_vreg();
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Load {
                        dst: tmp,
                        addr: addr.clone(),
                        width: self.size_to_memwidth(op_size),
                        sign: SignExtend::Zero,
                    },
                ));
                (tmp, self.gpr(modrm.reg))
            } else {
                // reg is destination, rm is source
                let tmp = ctx.alloc_vreg();
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Load {
                        dst: tmp,
                        addr,
                        width: self.size_to_memwidth(op_size),
                        sign: SignExtend::Zero,
                    },
                ));
                (self.gpr(modrm.reg), tmp)
            }
        } else if dir_rm_reg {
            (self.gpr(modrm.rm), self.gpr(modrm.reg))
        } else {
            (self.gpr(modrm.reg), self.gpr(modrm.rm))
        };

        // Determine operation from opcode high bits
        let result = dst;
        let hint = X86OpHint::AluEncoding(if dir_rm_reg {
            X86AluEncoding::RmReg
        } else {
            X86AluEncoding::RegRm
        });
        let op_kind = match (opcode >> 3) & 0x07 {
            0 => OpKind::Add {
                dst: result,
                src1: dst,
                src2: SrcOperand::Reg(src),
                width,
                flags: FlagUpdate::All,
            },
            1 => OpKind::Or {
                dst: result,
                src1: dst,
                src2: SrcOperand::Reg(src),
                width,
                flags: FlagUpdate::All,
            },
            2 => OpKind::Adc {
                dst: result,
                src1: dst,
                src2: SrcOperand::Reg(src),
                width,
                flags: FlagUpdate::All,
            },
            3 => OpKind::Sbb {
                dst: result,
                src1: dst,
                src2: SrcOperand::Reg(src),
                width,
                flags: FlagUpdate::All,
            },
            4 => OpKind::And {
                dst: result,
                src1: dst,
                src2: SrcOperand::Reg(src),
                width,
                flags: FlagUpdate::All,
            },
            5 => OpKind::Sub {
                dst: result,
                src1: dst,
                src2: SrcOperand::Reg(src),
                width,
                flags: FlagUpdate::All,
            },
            6 => OpKind::Xor {
                dst: result,
                src1: dst,
                src2: SrcOperand::Reg(src),
                width,
                flags: FlagUpdate::All,
            },
            7 => {
                // CMP - subtract but don't store
                ops.push(SmirOp::with_hint(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Cmp {
                        src1: dst,
                        src2: SrcOperand::Reg(src),
                        width,
                    },
                    hint,
                ));
                return Ok(LiftResult::fallthrough(
                    ops,
                    prefix.cursor + modrm.bytes_consumed,
                ));
            }
            _ => unreachable!(),
        };

        ops.push(SmirOp::with_hint(OpId(ops.len() as u16), pc, op_kind, hint));

        // Write back if destination was memory
        if modrm.is_memory && dir_rm_reg {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, _) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Store {
                    src: result,
                    addr,
                    width: self.size_to_memwidth(op_size),
                },
            ));
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed,
        ))
    }

    /// Lift Group 1 immediate instructions (80/81/83)
    fn lift_group1_imm(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let is_8bit = opcode == 0x80;
        let op_size = if is_8bit { 1 } else { prefix.op_size() };
        let width = self.size_to_width(op_size);
        let mem_width = self.size_to_memwidth(op_size);

        let modrm = decode_modrm(bytes, prefix, pc)?;
        let imm_offset = modrm.bytes_consumed;

        let (imm, imm_size) = match opcode {
            0x80 => {
                if bytes.len() < imm_offset + 1 {
                    return Err(LiftError::Incomplete {
                        addr: pc,
                        have: bytes.len(),
                        need: imm_offset + 1,
                    });
                }
                (bytes[imm_offset] as i8 as i64, 1)
            }
            0x81 => {
                let imm_size = if op_size == 2 { 2 } else { 4 };
                if bytes.len() < imm_offset + imm_size {
                    return Err(LiftError::Incomplete {
                        addr: pc,
                        have: bytes.len(),
                        need: imm_offset + imm_size,
                    });
                }
                let imm = if imm_size == 2 {
                    i16::from_le_bytes([bytes[imm_offset], bytes[imm_offset + 1]]) as i64
                } else {
                    i32::from_le_bytes([
                        bytes[imm_offset],
                        bytes[imm_offset + 1],
                        bytes[imm_offset + 2],
                        bytes[imm_offset + 3],
                    ]) as i64
                };
                (imm, imm_size)
            }
            0x83 => {
                if bytes.len() < imm_offset + 1 {
                    return Err(LiftError::Incomplete {
                        addr: pc,
                        have: bytes.len(),
                        need: imm_offset + 1,
                    });
                }
                (bytes[imm_offset] as i8 as i64, 1)
            }
            _ => unreachable!(),
        };

        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64 + imm_size as u64;
        let mut ops = Vec::new();

        let (dst, addr) = if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Load {
                    dst: tmp,
                    addr: addr.clone(),
                    width: mem_width,
                    sign: SignExtend::Zero,
                },
            ));
            (tmp, Some(addr))
        } else {
            (self.gpr(modrm.rm), None)
        };

        let group = (modrm.byte >> 3) & 0x07;
        match group {
            0 => ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Add {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
            )),
            1 => ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Or {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
            )),
            2 => ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Adc {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
            )),
            3 => ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Sbb {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
            )),
            4 => ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::And {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
            )),
            5 => ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Sub {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
            )),
            6 => ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Xor {
                    dst,
                    src1: dst,
                    src2: SrcOperand::Imm(imm),
                    width,
                    flags: FlagUpdate::All,
                },
            )),
            7 => {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Cmp {
                        src1: dst,
                        src2: SrcOperand::Imm(imm),
                        width,
                    },
                ));
            }
            _ => {}
        }

        if group != 7 {
            if let Some(addr) = addr {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Store {
                        src: dst,
                        addr,
                        width: mem_width,
                    },
                ));
            }
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed + imm_size,
        ))
    }

    /// Lift shift instructions with immediate (C0/C1)
    fn lift_shift_imm(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let op_size = if opcode == 0xC0 { 1 } else { prefix.op_size() };
        let width = self.size_to_width(op_size);

        let modrm = decode_modrm(bytes, prefix, pc)?;
        if bytes.len() < modrm.bytes_consumed + 1 {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: bytes.len(),
                need: modrm.bytes_consumed + 1,
            });
        }

        let imm = bytes[modrm.bytes_consumed] as i64;
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64 + 1;
        let mut ops = Vec::new();

        let group = (modrm.byte >> 3) & 0x07;

        let (src, addr) = if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Load {
                    dst: tmp,
                    addr: addr.clone(),
                    width: self.size_to_memwidth(op_size),
                    sign: SignExtend::Zero,
                },
            ));
            (tmp, Some(addr))
        } else {
            (self.gpr(modrm.rm), None)
        };

        let result = src;
        let op_kind = match group {
            0 => OpKind::Rol {
                dst: result,
                src,
                amount: SrcOperand::Imm(imm),
                width,
                flags: FlagUpdate::All,
            },
            1 => OpKind::Ror {
                dst: result,
                src,
                amount: SrcOperand::Imm(imm),
                width,
                flags: FlagUpdate::All,
            },
            4 | 6 => OpKind::Shl {
                dst: result,
                src,
                amount: SrcOperand::Imm(imm),
                width,
                flags: FlagUpdate::All,
            },
            5 => OpKind::Shr {
                dst: result,
                src,
                amount: SrcOperand::Imm(imm),
                width,
                flags: FlagUpdate::All,
            },
            7 => OpKind::Sar {
                dst: result,
                src,
                amount: SrcOperand::Imm(imm),
                width,
                flags: FlagUpdate::All,
            },
            _ => {
                if self.strict {
                    return Err(LiftError::Unsupported {
                        addr: pc,
                        mnemonic: format!("shift group {}", group),
                    });
                }
                return Ok(LiftResult::fallthrough(
                    vec![SmirOp::new(OpId(0), pc, OpKind::Nop)],
                    prefix.cursor + modrm.bytes_consumed + 1,
                ));
            }
        };

        ops.push(SmirOp::new(OpId(ops.len() as u16), pc, op_kind));

        if let Some(addr) = addr {
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Store {
                    src: result,
                    addr,
                    width: self.size_to_memwidth(op_size),
                },
            ));
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed + 1,
        ))
    }

    /// Lift shift instructions with implicit count = 1 (D0/D1)
    fn lift_shift_one(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let op_size = if opcode == 0xD0 { 1 } else { prefix.op_size() };
        let width = self.size_to_width(op_size);

        let modrm = decode_modrm(bytes, prefix, pc)?;
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;
        let mut ops = Vec::new();

        let group = (modrm.byte >> 3) & 0x07;

        let (src, addr) = if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Load {
                    dst: tmp,
                    addr: addr.clone(),
                    width: self.size_to_memwidth(op_size),
                    sign: SignExtend::Zero,
                },
            ));
            (tmp, Some(addr))
        } else {
            (self.gpr(modrm.rm), None)
        };

        let result = src;
        let op_kind = match group {
            0 => OpKind::Rol {
                dst: result,
                src,
                amount: SrcOperand::Imm(1),
                width,
                flags: FlagUpdate::All,
            },
            1 => OpKind::Ror {
                dst: result,
                src,
                amount: SrcOperand::Imm(1),
                width,
                flags: FlagUpdate::All,
            },
            4 | 6 => OpKind::Shl {
                dst: result,
                src,
                amount: SrcOperand::Imm(1),
                width,
                flags: FlagUpdate::All,
            },
            5 => OpKind::Shr {
                dst: result,
                src,
                amount: SrcOperand::Imm(1),
                width,
                flags: FlagUpdate::All,
            },
            7 => OpKind::Sar {
                dst: result,
                src,
                amount: SrcOperand::Imm(1),
                width,
                flags: FlagUpdate::All,
            },
            _ => {
                if self.strict {
                    return Err(LiftError::Unsupported {
                        addr: pc,
                        mnemonic: format!("shift group {}", group),
                    });
                }
                return Ok(LiftResult::fallthrough(
                    vec![SmirOp::new(OpId(0), pc, OpKind::Nop)],
                    prefix.cursor + modrm.bytes_consumed,
                ));
            }
        };

        ops.push(SmirOp::new(OpId(ops.len() as u16), pc, op_kind));

        if let Some(addr) = addr {
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Store {
                    src: result,
                    addr,
                    width: self.size_to_memwidth(op_size),
                },
            ));
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed,
        ))
    }

    /// Lift shift instructions with count in CL (D2/D3)
    fn lift_shift_cl(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let op_size = if opcode == 0xD2 { 1 } else { prefix.op_size() };
        let width = self.size_to_width(op_size);

        let modrm = decode_modrm(bytes, prefix, pc)?;
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;
        let mut ops = Vec::new();

        let group = (modrm.byte >> 3) & 0x07;

        let (src, addr) = if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Load {
                    dst: tmp,
                    addr: addr.clone(),
                    width: self.size_to_memwidth(op_size),
                    sign: SignExtend::Zero,
                },
            ));
            (tmp, Some(addr))
        } else {
            (self.gpr(modrm.rm), None)
        };

        let result = src;
        let amount = SrcOperand::Reg(self.gpr(1));
        let op_kind = match group {
            0 => OpKind::Rol {
                dst: result,
                src,
                amount,
                width,
                flags: FlagUpdate::All,
            },
            1 => OpKind::Ror {
                dst: result,
                src,
                amount,
                width,
                flags: FlagUpdate::All,
            },
            4 | 6 => OpKind::Shl {
                dst: result,
                src,
                amount,
                width,
                flags: FlagUpdate::All,
            },
            5 => OpKind::Shr {
                dst: result,
                src,
                amount,
                width,
                flags: FlagUpdate::All,
            },
            7 => OpKind::Sar {
                dst: result,
                src,
                amount,
                width,
                flags: FlagUpdate::All,
            },
            _ => {
                if self.strict {
                    return Err(LiftError::Unsupported {
                        addr: pc,
                        mnemonic: format!("shift group {}", group),
                    });
                }
                return Ok(LiftResult::fallthrough(
                    vec![SmirOp::new(OpId(0), pc, OpKind::Nop)],
                    prefix.cursor + modrm.bytes_consumed,
                ));
            }
        };

        ops.push(SmirOp::new(OpId(ops.len() as u16), pc, op_kind));

        if let Some(addr) = addr {
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Store {
                    src: result,
                    addr,
                    width: self.size_to_memwidth(op_size),
                },
            ));
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed,
        ))
    }

    /// Lift BSF/BSR (0F BC/0F BD)
    fn lift_bsf_bsr(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let op_size = prefix.op_size();
        let width = self.size_to_width(op_size);
        let mem_width = self.size_to_memwidth(op_size);
        let modrm = decode_modrm(bytes, prefix, pc)?;
        let mut ops = Vec::new();
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;

        let src = if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Load {
                    dst: tmp,
                    addr,
                    width: mem_width,
                    sign: SignExtend::Zero,
                },
            ));
            tmp
        } else {
            self.gpr(modrm.rm)
        };

        let op_kind = if opcode == 0xBC {
            OpKind::Bsf {
                dst: self.gpr(modrm.reg),
                src,
                width,
                flags: FlagUpdate::All,
            }
        } else {
            OpKind::Bsr {
                dst: self.gpr(modrm.reg),
                src,
                width,
                flags: FlagUpdate::All,
            }
        };

        ops.push(SmirOp::new(OpId(ops.len() as u16), pc, op_kind));

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed,
        ))
    }

    /// Lift SSE MOVDQA/MOVDQU (0F 6F/7F with prefixes)
    fn lift_sse_mov(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let modrm = decode_modrm(bytes, prefix, pc)?;
        let mut ops = Vec::new();
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;

        let prefix_kind = if prefix.rep_prefix == Some(0xF3) {
            X86SsePrefix::Rep
        } else if prefix.rep_prefix == Some(0xF2) {
            X86SsePrefix::Repne
        } else if prefix.operand_size_override {
            X86SsePrefix::OpSize
        } else {
            X86SsePrefix::None
        };

        let hint = X86OpHint::SseMov {
            prefix: prefix_kind,
            opcode,
        };

        match opcode {
            0x6F => {
                if modrm.is_memory {
                    let x86_addr = modrm.addr.as_ref().unwrap();
                    let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                    ops.extend(pre_ops);
                    ops.push(SmirOp::with_hint(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::VLoad {
                            dst: self.xmm(modrm.reg),
                            addr,
                            width: VecWidth::V128,
                        },
                        hint,
                    ));
                } else {
                    ops.push(SmirOp::with_hint(
                        OpId(0),
                        pc,
                        OpKind::VMov {
                            dst: self.xmm(modrm.reg),
                            src: self.xmm(modrm.rm),
                            width: VecWidth::V128,
                        },
                        hint,
                    ));
                }
            }
            0x7F => {
                if modrm.is_memory {
                    let x86_addr = modrm.addr.as_ref().unwrap();
                    let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                    ops.extend(pre_ops);
                    ops.push(SmirOp::with_hint(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::VStore {
                            src: self.xmm(modrm.reg),
                            addr,
                            width: VecWidth::V128,
                        },
                        hint,
                    ));
                } else {
                    ops.push(SmirOp::with_hint(
                        OpId(0),
                        pc,
                        OpKind::VMov {
                            dst: self.xmm(modrm.rm),
                            src: self.xmm(modrm.reg),
                            width: VecWidth::V128,
                        },
                        hint,
                    ));
                }
            }
            _ => {}
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed,
        ))
    }

    /// Lift SSE PADDD (66 0F FE)
    fn lift_sse_padd(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let modrm = decode_modrm(bytes, prefix, pc)?;
        let mut ops = Vec::new();
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;

        let prefix_kind = if prefix.rep_prefix == Some(0xF3) {
            X86SsePrefix::Rep
        } else if prefix.rep_prefix == Some(0xF2) {
            X86SsePrefix::Repne
        } else if prefix.operand_size_override {
            X86SsePrefix::OpSize
        } else {
            X86SsePrefix::None
        };

        if prefix_kind != X86SsePrefix::OpSize {
            if self.strict {
                return Err(LiftError::Unsupported {
                    addr: pc,
                    mnemonic: format!("sse opcode 0x{:02X}", opcode),
                });
            }
            return Ok(LiftResult::fallthrough(
                vec![SmirOp::new(OpId(0), pc, OpKind::Nop)],
                prefix.cursor + modrm.bytes_consumed,
            ));
        }

        let hint = X86OpHint::SseOp {
            prefix: prefix_kind,
            opcode,
        };

        let dst = self.xmm(modrm.reg);
        if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::VLoad {
                    dst: tmp,
                    addr,
                    width: VecWidth::V128,
                },
            ));
            ops.push(SmirOp::with_hint(
                OpId(ops.len() as u16),
                pc,
                OpKind::VAdd {
                    dst,
                    src1: dst,
                    src2: tmp,
                    elem: VecElementType::I32,
                    lanes: 4,
                },
                hint,
            ));
        } else {
            ops.push(SmirOp::with_hint(
                OpId(0),
                pc,
                OpKind::VAdd {
                    dst,
                    src1: dst,
                    src2: self.xmm(modrm.rm),
                    elem: VecElementType::I32,
                    lanes: 4,
                },
                hint,
            ));
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed,
        ))
    }

    /// Lift SSE4.1 PMULLD (66 0F 38 40)
    fn lift_sse_pmulld(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let modrm = decode_modrm(bytes, prefix, pc)?;
        let mut ops = Vec::new();
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;

        let prefix_kind = if prefix.rep_prefix == Some(0xF3) {
            X86SsePrefix::Rep
        } else if prefix.rep_prefix == Some(0xF2) {
            X86SsePrefix::Repne
        } else if prefix.operand_size_override {
            X86SsePrefix::OpSize
        } else {
            X86SsePrefix::None
        };

        if prefix_kind != X86SsePrefix::OpSize {
            if self.strict {
                return Err(LiftError::Unsupported {
                    addr: pc,
                    mnemonic: format!("sse opcode 0x{:02X}", opcode),
                });
            }
            return Ok(LiftResult::fallthrough(
                vec![SmirOp::new(OpId(0), pc, OpKind::Nop)],
                prefix.cursor + modrm.bytes_consumed,
            ));
        }

        let dst = self.xmm(modrm.reg);
        if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::VLoad {
                    dst: tmp,
                    addr,
                    width: VecWidth::V128,
                },
            ));
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::VMul {
                    dst,
                    src1: dst,
                    src2: tmp,
                    elem: VecElementType::I32,
                    lanes: 4,
                },
            ));
        } else {
            ops.push(SmirOp::new(
                OpId(0),
                pc,
                OpKind::VMul {
                    dst,
                    src1: dst,
                    src2: self.xmm(modrm.rm),
                    elem: VecElementType::I32,
                    lanes: 4,
                },
            ));
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed,
        ))
    }

    /// Lift 0F 38-prefixed (three-byte) opcodes
    fn lift_0f38_opcode(
        &self,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        if bytes.is_empty() {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: prefix.cursor + 1,
                need: prefix.cursor + 2,
            });
        }

        let opcode3 = bytes[0];
        let after_opcode = &bytes[1..];
        let prefix3 = X86Prefix {
            cursor: prefix.cursor + 1,
            ..prefix.clone()
        };

        match opcode3 {
            0x40 => self.lift_sse_pmulld(opcode3, after_opcode, &prefix3, pc, ctx),
            _ => {
                if self.strict {
                    Err(LiftError::Unsupported {
                        addr: pc,
                        mnemonic: format!("0x0F 0x38 0x{:02X}", opcode3),
                    })
                } else {
                    Ok(LiftResult::fallthrough(
                        vec![SmirOp::new(OpId(0), pc, OpKind::Nop)],
                        prefix3.cursor,
                    ))
                }
            }
        }
    }

    fn vec_hint(&self, prefix: VecPrefix, opcode: u8) -> X86OpHint {
        match prefix.encoding {
            VecEncodingKind::Vex => X86OpHint::VexOp {
                map: prefix.map,
                pp: prefix.pp,
                opcode,
                width: prefix.width,
            },
            VecEncodingKind::Evex => X86OpHint::EvexOp {
                map: prefix.map,
                pp: prefix.pp,
                opcode,
                width: prefix.width,
            },
        }
    }

    fn lift_vec_opcode(
        &self,
        prefix: VecPrefix,
        bytes: &[u8],
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        if bytes.len() < prefix.bytes + 1 {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: bytes.len(),
                need: prefix.bytes + 1,
            });
        }

        let opcode = bytes[prefix.bytes];
        let after_opcode = &bytes[prefix.bytes + 1..];
        let cursor = prefix.bytes + 1;
        let prefix_modrm = X86Prefix {
            rex: prefix.rex,
            operand_size_override: matches!(prefix.pp, X86SsePrefix::OpSize),
            rep_prefix: match prefix.pp {
                X86SsePrefix::Rep => Some(0xF3),
                X86SsePrefix::Repne => Some(0xF2),
                _ => None,
            },
            cursor,
            ..X86Prefix::default()
        };

        let mut ops = Vec::new();
        let hint = self.vec_hint(prefix, opcode);

        match prefix.map {
            X86VecMap::Map0F => match opcode {
                // VMOVAPS (0F 28/29) and VMOVDQA (0F 6F/7F with 66)
                0x28 | 0x29 | 0x6F | 0x7F => {
                    let modrm = decode_modrm(after_opcode, &prefix_modrm, pc)?;
                    let next_pc = pc + cursor as u64 + modrm.bytes_consumed as u64;
                    let dst_reg = self.vec_reg(modrm.reg, prefix.width);
                    let rm_reg = self.vec_reg(modrm.rm, prefix.width);

                    match opcode {
                        0x28 | 0x6F => {
                            if modrm.is_memory {
                                let x86_addr = modrm.addr.as_ref().unwrap();
                                let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                                ops.extend(pre_ops);
                                ops.push(SmirOp::with_hint(
                                    OpId(ops.len() as u16),
                                    pc,
                                    OpKind::VLoad {
                                        dst: dst_reg,
                                        addr,
                                        width: prefix.width,
                                    },
                                    hint,
                                ));
                            } else {
                                ops.push(SmirOp::with_hint(
                                    OpId(0),
                                    pc,
                                    OpKind::VMov {
                                        dst: dst_reg,
                                        src: rm_reg,
                                        width: prefix.width,
                                    },
                                    hint,
                                ));
                            }
                        }
                        0x29 | 0x7F => {
                            if modrm.is_memory {
                                let x86_addr = modrm.addr.as_ref().unwrap();
                                let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                                ops.extend(pre_ops);
                                ops.push(SmirOp::with_hint(
                                    OpId(ops.len() as u16),
                                    pc,
                                    OpKind::VStore {
                                        src: dst_reg,
                                        addr,
                                        width: prefix.width,
                                    },
                                    hint,
                                ));
                            } else {
                                ops.push(SmirOp::with_hint(
                                    OpId(0),
                                    pc,
                                    OpKind::VMov {
                                        dst: rm_reg,
                                        src: dst_reg,
                                        width: prefix.width,
                                    },
                                    hint,
                                ));
                            }
                        }
                        _ => {}
                    }

                    Ok(LiftResult::fallthrough(ops, cursor + modrm.bytes_consumed))
                }

                // VADDPS/VADDPS/VMULPS/VMAXPS
                0x58 | 0x59 | 0x5C | 0x5F | 0xFE => {
                    let modrm = decode_modrm(after_opcode, &prefix_modrm, pc)?;
                    let next_pc = pc + cursor as u64 + modrm.bytes_consumed as u64;
                    let dst = self.vec_reg(modrm.reg, prefix.width);
                    let src1 = self.vec_reg(prefix.vvvv, prefix.width);

                    let (elem, lanes) = if opcode == 0xFE {
                        (
                            VecElementType::I32,
                            prefix.width.lanes(VecElementType::I32) as u8,
                        )
                    } else {
                        (
                            VecElementType::F32,
                            prefix.width.lanes(VecElementType::F32) as u8,
                        )
                    };

                    let src2 = if modrm.is_memory {
                        let x86_addr = modrm.addr.as_ref().unwrap();
                        let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                        ops.extend(pre_ops);
                        let tmp = ctx.alloc_vreg();
                        ops.push(SmirOp::new(
                            OpId(ops.len() as u16),
                            pc,
                            OpKind::VLoad {
                                dst: tmp,
                                addr,
                                width: prefix.width,
                            },
                        ));
                        tmp
                    } else {
                        self.vec_reg(modrm.rm, prefix.width)
                    };

                    let op_kind = match opcode {
                        0x58 | 0xFE => OpKind::VAdd {
                            dst,
                            src1,
                            src2,
                            elem,
                            lanes,
                        },
                        0x5C => OpKind::VSub {
                            dst,
                            src1,
                            src2,
                            elem,
                            lanes,
                        },
                        0x59 => OpKind::VMul {
                            dst,
                            src1,
                            src2,
                            elem,
                            lanes,
                        },
                        0x5F => OpKind::VMax {
                            dst,
                            src1,
                            src2,
                            elem,
                            lanes,
                        },
                        _ => {
                            return Err(LiftError::Unsupported {
                                addr: pc,
                                mnemonic: format!("VEX opcode 0x{:02X}", opcode),
                            })
                        }
                    };

                    ops.push(SmirOp::with_hint(OpId(ops.len() as u16), pc, op_kind, hint));

                    Ok(LiftResult::fallthrough(ops, cursor + modrm.bytes_consumed))
                }

                // VPSLLD imm8 (0F 72 /6)
                0x72 => {
                    let modrm = decode_modrm(after_opcode, &prefix_modrm, pc)?;
                    let imm_offset = modrm.bytes_consumed;
                    if after_opcode.len() <= imm_offset {
                        return Err(LiftError::Incomplete {
                            addr: pc,
                            have: bytes.len(),
                            need: cursor + imm_offset + 1,
                        });
                    }
                    let imm = after_opcode[imm_offset];
                    let next_pc = pc + cursor as u64 + modrm.bytes_consumed as u64 + 1;

                    let group = (modrm.byte >> 3) & 0x07;
                    if group != 6 {
                        return Err(LiftError::Unsupported {
                            addr: pc,
                            mnemonic: format!("VEX shift group {}", group),
                        });
                    }

                    let dst = self.vec_reg(prefix.vvvv, prefix.width);
                    let src = if modrm.is_memory {
                        let x86_addr = modrm.addr.as_ref().unwrap();
                        let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                        ops.extend(pre_ops);
                        let tmp = ctx.alloc_vreg();
                        ops.push(SmirOp::new(
                            OpId(ops.len() as u16),
                            pc,
                            OpKind::VLoad {
                                dst: tmp,
                                addr,
                                width: prefix.width,
                            },
                        ));
                        tmp
                    } else {
                        self.vec_reg(modrm.rm, prefix.width)
                    };

                    ops.push(SmirOp::with_hint(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::VShift {
                            dst,
                            src,
                            amount: SrcOperand::Imm(imm as i64),
                            shift: ShiftOp::Lsl,
                            elem: VecElementType::I32,
                            lanes: prefix.width.lanes(VecElementType::I32) as u8,
                        },
                        hint,
                    ));

                    Ok(LiftResult::fallthrough(
                        ops,
                        cursor + modrm.bytes_consumed + 1,
                    ))
                }

                _ => Err(LiftError::Unsupported {
                    addr: pc,
                    mnemonic: format!("VEX opcode 0x{:02X}", opcode),
                }),
            },
            X86VecMap::Map0F38 => match opcode {
                0x40 => {
                    let modrm = decode_modrm(after_opcode, &prefix_modrm, pc)?;
                    let next_pc = pc + cursor as u64 + modrm.bytes_consumed as u64;
                    let dst = self.vec_reg(modrm.reg, prefix.width);
                    let src1 = self.vec_reg(prefix.vvvv, prefix.width);

                    let src2 = if modrm.is_memory {
                        let x86_addr = modrm.addr.as_ref().unwrap();
                        let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                        ops.extend(pre_ops);
                        let tmp = ctx.alloc_vreg();
                        ops.push(SmirOp::new(
                            OpId(ops.len() as u16),
                            pc,
                            OpKind::VLoad {
                                dst: tmp,
                                addr,
                                width: prefix.width,
                            },
                        ));
                        tmp
                    } else {
                        self.vec_reg(modrm.rm, prefix.width)
                    };

                    ops.push(SmirOp::with_hint(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::VMul {
                            dst,
                            src1,
                            src2,
                            elem: VecElementType::I32,
                            lanes: prefix.width.lanes(VecElementType::I32) as u8,
                        },
                        hint,
                    ));

                    Ok(LiftResult::fallthrough(ops, cursor + modrm.bytes_consumed))
                }
                _ => Err(LiftError::Unsupported {
                    addr: pc,
                    mnemonic: format!("VEX 0F38 opcode 0x{:02X}", opcode),
                }),
            },
            X86VecMap::Map0F3A => Err(LiftError::Unsupported {
                addr: pc,
                mnemonic: "VEX 0F3A".to_string(),
            }),
        }
    }

    fn lift_vex_evex(
        &self,
        pc: u64,
        bytes: &[u8],
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let prefix = match bytes.first().copied() {
            Some(0x62) => decode_evex_prefix(bytes, pc)?,
            _ => decode_vex_prefix(bytes, pc)?,
        };

        self.lift_vec_opcode(prefix, bytes, pc, ctx)
    }

    /// Lift group 5 instructions (FF)
    fn lift_group5(
        &self,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let op_size = prefix.op_size();
        let width = self.size_to_width(op_size);
        let mem_width = self.size_to_memwidth(op_size);

        let modrm = decode_modrm(bytes, prefix, pc)?;
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;
        let mut ops = Vec::new();

        let group = (modrm.byte >> 3) & 0x07;

        if modrm.is_memory && (group == 2 || group == 4) {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);
            let control_flow = if group == 2 {
                ControlFlow::Call {
                    target: CallTarget::IndirectMem(addr),
                }
            } else {
                ControlFlow::IndirectBranchMem { addr }
            };

            return Ok(LiftResult {
                ops,
                bytes_consumed: prefix.cursor + modrm.bytes_consumed,
                control_flow,
                branch_targets: vec![],
            });
        }

        let (operand, addr) = if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Load {
                    dst: tmp,
                    addr: addr.clone(),
                    width: mem_width,
                    sign: SignExtend::Zero,
                },
            ));
            (tmp, Some(addr))
        } else {
            (self.gpr(modrm.rm), None)
        };

        match group {
            0 => {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Inc {
                        dst: operand,
                        src: operand,
                        width,
                        flags: FlagUpdate::All,
                    },
                ));
                if let Some(addr) = addr {
                    ops.push(SmirOp::new(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::Store {
                            src: operand,
                            addr,
                            width: mem_width,
                        },
                    ));
                }
                Ok(LiftResult::fallthrough(
                    ops,
                    prefix.cursor + modrm.bytes_consumed,
                ))
            }
            1 => {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Dec {
                        dst: operand,
                        src: operand,
                        width,
                        flags: FlagUpdate::All,
                    },
                ));
                if let Some(addr) = addr {
                    ops.push(SmirOp::new(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::Store {
                            src: operand,
                            addr,
                            width: mem_width,
                        },
                    ));
                }
                Ok(LiftResult::fallthrough(
                    ops,
                    prefix.cursor + modrm.bytes_consumed,
                ))
            }
            2 => Ok(LiftResult {
                ops,
                bytes_consumed: prefix.cursor + modrm.bytes_consumed,
                control_flow: ControlFlow::Call {
                    target: CallTarget::Indirect(operand),
                },
                branch_targets: vec![],
            }),
            4 => Ok(LiftResult {
                ops,
                bytes_consumed: prefix.cursor + modrm.bytes_consumed,
                control_flow: ControlFlow::IndirectBranch { target: operand },
                branch_targets: vec![],
            }),
            6 => {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Sub {
                        dst: self.rsp(),
                        src1: self.rsp(),
                        src2: SrcOperand::Imm(8),
                        width: OpWidth::W64,
                        flags: FlagUpdate::None,
                    },
                ));
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Store {
                        src: operand,
                        addr: Address::Direct(self.rsp()),
                        width: MemWidth::B8,
                    },
                ));
                Ok(LiftResult::fallthrough(
                    ops,
                    prefix.cursor + modrm.bytes_consumed,
                ))
            }
            _ => {
                if self.strict {
                    Err(LiftError::Unsupported {
                        addr: pc,
                        mnemonic: format!("group5 {}", group),
                    })
                } else {
                    Ok(LiftResult::fallthrough(
                        vec![SmirOp::new(OpId(0), pc, OpKind::Nop)],
                        prefix.cursor + modrm.bytes_consumed,
                    ))
                }
            }
        }
    }

    /// Lift group 3 instructions (F6/F7)
    fn lift_group3(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let is_8bit = opcode == 0xF6;
        let op_size = if is_8bit { 1 } else { prefix.op_size() };
        let width = self.size_to_width(op_size);
        let mem_width = self.size_to_memwidth(op_size);

        let modrm = decode_modrm(bytes, prefix, pc)?;
        let group = (modrm.byte >> 3) & 0x07;
        let imm_size = if group == 0 {
            if is_8bit {
                1
            } else if op_size == 2 {
                2
            } else {
                4
            }
        } else {
            0
        };

        if bytes.len() < modrm.bytes_consumed + imm_size {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: bytes.len(),
                need: modrm.bytes_consumed + imm_size,
            });
        }

        let imm = if imm_size == 0 {
            0
        } else if imm_size == 1 {
            bytes[modrm.bytes_consumed] as i8 as i64
        } else if imm_size == 2 {
            i16::from_le_bytes([bytes[modrm.bytes_consumed], bytes[modrm.bytes_consumed + 1]])
                as i64
        } else {
            i32::from_le_bytes([
                bytes[modrm.bytes_consumed],
                bytes[modrm.bytes_consumed + 1],
                bytes[modrm.bytes_consumed + 2],
                bytes[modrm.bytes_consumed + 3],
            ]) as i64
        };

        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64 + imm_size as u64;
        let mut ops = Vec::new();

        let (operand, addr) = if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Load {
                    dst: tmp,
                    addr: addr.clone(),
                    width: mem_width,
                    sign: SignExtend::Zero,
                },
            ));
            (tmp, Some(addr))
        } else {
            (self.gpr(modrm.rm), None)
        };

        match group {
            0 => {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Test {
                        src1: operand,
                        src2: SrcOperand::Imm(imm),
                        width,
                    },
                ));
            }
            2 => {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Not {
                        dst: operand,
                        src: operand,
                        width,
                    },
                ));
            }
            3 => {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Neg {
                        dst: operand,
                        src: operand,
                        width,
                        flags: FlagUpdate::All,
                    },
                ));
            }
            4 => {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::MulU {
                        dst_lo: self.gpr(0),
                        dst_hi: Some(self.gpr(2)),
                        src1: self.gpr(0),
                        src2: SrcOperand::Reg(operand),
                        width,
                        flags: FlagUpdate::All,
                    },
                ));
            }
            5 => {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::MulS {
                        dst_lo: self.gpr(0),
                        dst_hi: Some(self.gpr(2)),
                        src1: self.gpr(0),
                        src2: SrcOperand::Reg(operand),
                        width,
                        flags: FlagUpdate::All,
                    },
                ));
            }
            6 => {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::DivU {
                        quot: self.gpr(0),
                        rem: Some(self.gpr(2)),
                        src1: self.gpr(0),
                        src2: SrcOperand::Reg(operand),
                        width,
                    },
                ));
            }
            7 => {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::DivS {
                        quot: self.gpr(0),
                        rem: Some(self.gpr(2)),
                        src1: self.gpr(0),
                        src2: SrcOperand::Reg(operand),
                        width,
                    },
                ));
            }
            _ => {
                if self.strict {
                    return Err(LiftError::Unsupported {
                        addr: pc,
                        mnemonic: format!("group3 {}", group),
                    });
                }
                ops.push(SmirOp::new(OpId(ops.len() as u16), pc, OpKind::Nop));
            }
        }

        if matches!(group, 2 | 3) {
            if let Some(addr) = addr {
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Store {
                        src: operand,
                        addr,
                        width: mem_width,
                    },
                ));
            }
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed + imm_size,
        ))
    }

    /// Lift IMUL r, r/m, imm (69/6B)
    fn lift_imul_rmi(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let op_size = prefix.op_size();
        let width = self.size_to_width(op_size);
        let modrm = decode_modrm(bytes, prefix, pc)?;
        let imm_offset = modrm.bytes_consumed;
        let imm_size = if opcode == 0x6B {
            1
        } else if op_size == 2 {
            2
        } else {
            4
        };

        if bytes.len() < imm_offset + imm_size {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: bytes.len(),
                need: imm_offset + imm_size,
            });
        }

        let imm = match imm_size {
            1 => bytes[imm_offset] as i8 as i64,
            2 => i16::from_le_bytes([bytes[imm_offset], bytes[imm_offset + 1]]) as i64,
            _ => i32::from_le_bytes([
                bytes[imm_offset],
                bytes[imm_offset + 1],
                bytes[imm_offset + 2],
                bytes[imm_offset + 3],
            ]) as i64,
        };

        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64 + imm_size as u64;
        let mut ops = Vec::new();

        let src = if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Load {
                    dst: tmp,
                    addr,
                    width: self.size_to_memwidth(op_size),
                    sign: SignExtend::Zero,
                },
            ));
            tmp
        } else {
            self.gpr(modrm.rm)
        };

        let hint = if opcode == 0x6B {
            X86OpHint::ImulImm8
        } else {
            X86OpHint::ImulImm32
        };

        ops.push(SmirOp::with_hint(
            OpId(ops.len() as u16),
            pc,
            OpKind::MulS {
                dst_lo: self.gpr(modrm.reg),
                dst_hi: None,
                src1: src,
                src2: SrcOperand::Imm(imm),
                width,
                flags: FlagUpdate::All,
            },
            hint,
        ));

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed + imm_size,
        ))
    }

    /// Lift SHLD/SHRD (0F A4/A5/AC/AD)
    fn lift_shld_shrd(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let uses_cl = opcode == 0xA5 || opcode == 0xAD;
        let is_shld = opcode == 0xA4 || opcode == 0xA5;

        let op_size = prefix.op_size();
        let width = self.size_to_width(op_size);
        let mem_width = self.size_to_memwidth(op_size);

        let modrm = decode_modrm(bytes, prefix, pc)?;
        let imm_size = if uses_cl { 0 } else { 1 };
        if bytes.len() < modrm.bytes_consumed + imm_size {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: bytes.len(),
                need: modrm.bytes_consumed + imm_size,
            });
        }

        let imm = if uses_cl {
            0
        } else {
            bytes[modrm.bytes_consumed] as i8 as i64
        };

        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64 + imm_size as u64;
        let mut ops = Vec::new();

        let (dst, addr) = if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Load {
                    dst: tmp,
                    addr: addr.clone(),
                    width: mem_width,
                    sign: SignExtend::Zero,
                },
            ));
            (tmp, Some(addr))
        } else {
            (self.gpr(modrm.rm), None)
        };

        let amount = if uses_cl {
            SrcOperand::Reg(self.gpr(1))
        } else {
            SrcOperand::Imm(imm)
        };

        let op_kind = if is_shld {
            OpKind::Shld {
                dst,
                src: self.gpr(modrm.reg),
                amount,
                width,
                flags: FlagUpdate::All,
            }
        } else {
            OpKind::Shrd {
                dst,
                src: self.gpr(modrm.reg),
                amount,
                width,
                flags: FlagUpdate::All,
            }
        };

        ops.push(SmirOp::new(OpId(ops.len() as u16), pc, op_kind));

        if let Some(addr) = addr {
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Store {
                    src: dst,
                    addr,
                    width: mem_width,
                },
            ));
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed + imm_size,
        ))
    }

    /// Lift MOV r, imm (B8-BF)
    fn lift_mov_r_imm(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        _ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let reg = (opcode & 0x07) | prefix.rex_b();
        let op_size = prefix.op_size();
        let width = self.size_to_width(op_size);

        // In 64-bit mode with REX.W, we can have 64-bit immediate
        let (imm, imm_size): (i64, usize) = if prefix.rex_w() {
            if bytes.len() < 8 {
                return Err(LiftError::Incomplete {
                    addr: pc,
                    have: bytes.len(),
                    need: 8,
                });
            }
            (
                i64::from_le_bytes([
                    bytes[0], bytes[1], bytes[2], bytes[3], bytes[4], bytes[5], bytes[6], bytes[7],
                ]),
                8,
            )
        } else {
            match op_size {
                2 => {
                    if bytes.len() < 2 {
                        return Err(LiftError::Incomplete {
                            addr: pc,
                            have: bytes.len(),
                            need: 2,
                        });
                    }
                    (i16::from_le_bytes([bytes[0], bytes[1]]) as i64, 2)
                }
                _ => {
                    if bytes.len() < 4 {
                        return Err(LiftError::Incomplete {
                            addr: pc,
                            have: bytes.len(),
                            need: 4,
                        });
                    }
                    (
                        i32::from_le_bytes([bytes[0], bytes[1], bytes[2], bytes[3]]) as i64,
                        4,
                    )
                }
            }
        };

        let src = if prefix.rex_w() {
            SrcOperand::Imm64(imm)
        } else {
            SrcOperand::Imm(imm)
        };

        let ops = vec![SmirOp::new(
            OpId(0),
            pc,
            OpKind::Mov {
                dst: self.gpr(reg),
                src,
                width,
            },
        )];

        Ok(LiftResult::fallthrough(ops, prefix.cursor + imm_size))
    }

    /// Lift MOV r8, imm8 (B0-B7)
    fn lift_mov_r8_imm8(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        _ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let reg = (opcode & 0x07) | prefix.rex_b();

        if bytes.is_empty() {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: 0,
                need: 1,
            });
        }

        let imm = bytes[0] as i8 as i64;

        let ops = vec![SmirOp::new(
            OpId(0),
            pc,
            OpKind::Mov {
                dst: self.gpr(reg),
                src: SrcOperand::Imm(imm),
                width: OpWidth::W8,
            },
        )];

        Ok(LiftResult::fallthrough(ops, prefix.cursor + 1))
    }

    /// Lift PUSH r64 (50-57)
    fn lift_push_r64(
        &self,
        opcode: u8,
        prefix: &X86Prefix,
        pc: u64,
        _ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let reg = (opcode & 0x07) | prefix.rex_b();
        let mut ops = Vec::new();

        // RSP -= 8
        ops.push(SmirOp::new(
            OpId(0),
            pc,
            OpKind::Sub {
                dst: self.rsp(),
                src1: self.rsp(),
                src2: SrcOperand::Imm(8),
                width: OpWidth::W64,
                flags: FlagUpdate::None,
            },
        ));

        // [RSP] = reg
        ops.push(SmirOp::new(
            OpId(1),
            pc,
            OpKind::Store {
                src: self.gpr(reg),
                addr: Address::Direct(self.rsp()),
                width: MemWidth::B8,
            },
        ));

        Ok(LiftResult::fallthrough(ops, prefix.cursor))
    }

    /// Lift POP r64 (58-5F)
    fn lift_pop_r64(
        &self,
        opcode: u8,
        prefix: &X86Prefix,
        pc: u64,
        _ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let reg = (opcode & 0x07) | prefix.rex_b();
        let mut ops = Vec::new();

        // reg = [RSP]
        ops.push(SmirOp::new(
            OpId(0),
            pc,
            OpKind::Load {
                dst: self.gpr(reg),
                addr: Address::Direct(self.rsp()),
                width: MemWidth::B8,
                sign: SignExtend::Zero,
            },
        ));

        // RSP += 8
        ops.push(SmirOp::new(
            OpId(1),
            pc,
            OpKind::Add {
                dst: self.rsp(),
                src1: self.rsp(),
                src2: SrcOperand::Imm(8),
                width: OpWidth::W64,
                flags: FlagUpdate::None,
            },
        ));

        Ok(LiftResult::fallthrough(ops, prefix.cursor))
    }

    /// Lift CWD/CDQ/CQO (99)
    fn lift_cwd_cdq_cqo(&self, prefix: &X86Prefix, pc: u64) -> Result<LiftResult, LiftError> {
        let width = self.size_to_width(prefix.op_size());
        let ops = vec![SmirOp::new(
            OpId(0),
            pc,
            OpKind::Cwd {
                dst: self.gpr(2),
                src: self.gpr(0),
                width,
            },
        )];

        Ok(LiftResult::fallthrough(ops, prefix.cursor))
    }

    /// Lift XCHG rax, r64 (90-97)
    fn lift_xchg_rax(
        &self,
        opcode: u8,
        prefix: &X86Prefix,
        pc: u64,
    ) -> Result<LiftResult, LiftError> {
        let reg = (opcode & 0x07) | prefix.rex_b();
        let width = self.size_to_width(prefix.op_size());
        let ops = vec![SmirOp::new(
            OpId(0),
            pc,
            OpKind::Xchg {
                reg1: self.gpr(0),
                reg2: self.gpr(reg),
                width,
            },
        )];

        Ok(LiftResult::fallthrough(ops, prefix.cursor))
    }

    /// Lift PUSH imm8/imm32 (6A/68)
    fn lift_push_imm(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
    ) -> Result<LiftResult, LiftError> {
        let imm_size = if opcode == 0x6A { 1 } else { 4 };
        if bytes.len() < imm_size {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: bytes.len(),
                need: imm_size,
            });
        }

        let imm = if imm_size == 1 {
            bytes[0] as i8 as i64
        } else {
            i32::from_le_bytes([bytes[0], bytes[1], bytes[2], bytes[3]]) as i64
        };

        let hint = if imm_size == 1 {
            X86OpHint::PushImm8
        } else {
            X86OpHint::PushImm32
        };

        let mut ops = Vec::new();
        ops.push(SmirOp::new(
            OpId(0),
            pc,
            OpKind::Sub {
                dst: self.rsp(),
                src1: self.rsp(),
                src2: SrcOperand::Imm(8),
                width: OpWidth::W64,
                flags: FlagUpdate::None,
            },
        ));

        ops.push(SmirOp::with_hint(
            OpId(1),
            pc,
            OpKind::Store {
                src: VReg::Imm(imm),
                addr: Address::Direct(self.rsp()),
                width: MemWidth::B8,
            },
            hint,
        ));

        Ok(LiftResult::fallthrough(ops, prefix.cursor + imm_size))
    }

    /// Lift CALL rel32 (E8)
    fn lift_call_rel32(
        &self,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        _ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        if bytes.len() < 4 {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: bytes.len(),
                need: 4,
            });
        }

        let rel = i32::from_le_bytes([bytes[0], bytes[1], bytes[2], bytes[3]]) as i64;
        let insn_len = prefix.cursor + 4;
        let next_rip = pc + insn_len as u64;
        let target = (next_rip as i64 + rel) as u64;

        Ok(LiftResult {
            ops: vec![],
            bytes_consumed: insn_len,
            control_flow: ControlFlow::Call {
                target: CallTarget::GuestAddr(target),
            },
            branch_targets: vec![target],
        })
    }

    /// Lift RET (C3)
    fn lift_ret(
        &self,
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let mut ops = Vec::new();
        let ret_addr = ctx.alloc_vreg();

        // Load return address
        ops.push(SmirOp::new(
            OpId(0),
            pc,
            OpKind::Load {
                dst: ret_addr,
                addr: Address::Direct(self.rsp()),
                width: MemWidth::B8,
                sign: SignExtend::Zero,
            },
        ));

        // RSP += 8
        ops.push(SmirOp::new(
            OpId(1),
            pc,
            OpKind::Add {
                dst: self.rsp(),
                src1: self.rsp(),
                src2: SrcOperand::Imm(8),
                width: OpWidth::W64,
                flags: FlagUpdate::None,
            },
        ));

        Ok(LiftResult::ret(ops, prefix.cursor))
    }

    /// Lift RET imm16 (C2)
    fn lift_ret_imm16(
        &self,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        if bytes.len() < 2 {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: bytes.len(),
                need: 2,
            });
        }

        let imm = u16::from_le_bytes([bytes[0], bytes[1]]) as i64;
        let mut ops = Vec::new();
        let ret_addr = ctx.alloc_vreg();

        ops.push(SmirOp::new(
            OpId(0),
            pc,
            OpKind::Load {
                dst: ret_addr,
                addr: Address::Direct(self.rsp()),
                width: MemWidth::B8,
                sign: SignExtend::Zero,
            },
        ));

        ops.push(SmirOp::new(
            OpId(1),
            pc,
            OpKind::Add {
                dst: self.rsp(),
                src1: self.rsp(),
                src2: SrcOperand::Imm(8 + imm),
                width: OpWidth::W64,
                flags: FlagUpdate::None,
            },
        ));

        Ok(LiftResult::ret(ops, prefix.cursor + 2))
    }

    /// Lift LEAVE (C9)
    fn lift_leave(&self, prefix: &X86Prefix, pc: u64) -> Result<LiftResult, LiftError> {
        let ops = vec![SmirOp::new(OpId(0), pc, OpKind::Leave)];
        Ok(LiftResult::fallthrough(ops, prefix.cursor))
    }

    /// Lift JMP rel8 (EB)
    fn lift_jmp_rel8(
        &self,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        _ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        if bytes.is_empty() {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: 0,
                need: 1,
            });
        }

        let rel = bytes[0] as i8 as i64;
        let insn_len = prefix.cursor + 1;
        let target = (pc as i64 + insn_len as i64 + rel) as u64;

        Ok(LiftResult::branch(vec![], insn_len, target))
    }

    /// Lift JMP rel32 (E9)
    fn lift_jmp_rel32(
        &self,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        _ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        if bytes.len() < 4 {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: bytes.len(),
                need: 4,
            });
        }

        let rel = i32::from_le_bytes([bytes[0], bytes[1], bytes[2], bytes[3]]) as i64;
        let insn_len = prefix.cursor + 4;
        let target = (pc as i64 + insn_len as i64 + rel) as u64;

        Ok(LiftResult::branch(vec![], insn_len, target))
    }

    /// Lift Jcc rel8 (70-7F)
    fn lift_jcc_rel8(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        _ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        if bytes.is_empty() {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: 0,
                need: 1,
            });
        }

        let cc = opcode & 0x0F;
        let cond = self.x86_cond(cc);
        let rel = bytes[0] as i8 as i64;
        let insn_len = prefix.cursor + 1;
        let next_pc = pc + insn_len as u64;
        let target = (next_pc as i64 + rel) as u64;

        Ok(LiftResult::cond_branch(
            vec![],
            insn_len,
            cond,
            target,
            next_pc,
        ))
    }

    /// Lift LEA (8D)
    fn lift_lea(
        &self,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let modrm = decode_modrm(bytes, prefix, pc)?;

        if !modrm.is_memory {
            return Err(LiftError::InvalidEncoding {
                addr: pc,
                bytes: bytes.to_vec(),
            });
        }

        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;

        let x86_addr = modrm.addr.as_ref().unwrap();
        let (addr, mut ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);

        ops.push(SmirOp::new(
            OpId(ops.len() as u16),
            pc,
            OpKind::Lea {
                dst: self.gpr(modrm.reg),
                addr,
            },
        ));

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed,
        ))
    }

    /// Lift STOS/REP STOS (AA/AB with optional REP prefix)
    fn lift_stos(
        &self,
        opcode: u8,
        prefix: &X86Prefix,
        pc: u64,
        _ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let width = match opcode {
            0xAA => MemWidth::B1,
            0xAB => self.size_to_memwidth(prefix.op_size()),
            _ => MemWidth::B1,
        };

        if prefix.rep_prefix == Some(0xF3) {
            let ops = vec![SmirOp::new(
                OpId(0),
                pc,
                OpKind::RepStos {
                    dst: self.gpr(7),
                    src: self.gpr(0),
                    count: self.gpr(1),
                    width,
                },
            )];

            Ok(LiftResult::fallthrough(ops, prefix.cursor))
        } else if self.strict {
            Err(LiftError::Unsupported {
                addr: pc,
                mnemonic: "stos".to_string(),
            })
        } else {
            Ok(LiftResult::fallthrough(vec![], prefix.cursor))
        }
    }

    /// Lift MOVS/REP MOVS (A4/A5 with optional REP prefix)
    fn lift_movs(&self, opcode: u8, prefix: &X86Prefix, pc: u64) -> Result<LiftResult, LiftError> {
        let width = match opcode {
            0xA4 => MemWidth::B1,
            0xA5 => self.size_to_memwidth(prefix.op_size()),
            _ => MemWidth::B1,
        };

        if prefix.rep_prefix.is_some() {
            let ops = vec![SmirOp::new(
                OpId(0),
                pc,
                OpKind::RepMovs {
                    dst: self.gpr(7),
                    src: self.gpr(6),
                    count: self.gpr(1),
                    width,
                },
            )];

            Ok(LiftResult::fallthrough(ops, prefix.cursor))
        } else if self.strict {
            Err(LiftError::Unsupported {
                addr: pc,
                mnemonic: "movs".to_string(),
            })
        } else {
            Ok(LiftResult::fallthrough(vec![], prefix.cursor))
        }
    }

    /// Lift IN imm8 or DX (E4/E5/EC/ED)
    fn lift_in(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
    ) -> Result<LiftResult, LiftError> {
        let (port, width, imm_len) = match opcode {
            0xE4 => {
                if bytes.is_empty() {
                    return Err(LiftError::Incomplete {
                        addr: pc,
                        have: 0,
                        need: 1,
                    });
                }
                (VReg::Imm(bytes[0] as i8 as i64), MemWidth::B1, 1)
            }
            0xE5 => {
                if bytes.is_empty() {
                    return Err(LiftError::Incomplete {
                        addr: pc,
                        have: 0,
                        need: 1,
                    });
                }
                let width = if prefix.operand_size_override {
                    MemWidth::B2
                } else {
                    MemWidth::B4
                };
                (VReg::Imm(bytes[0] as i8 as i64), width, 1)
            }
            0xEC => (self.gpr(2), MemWidth::B1, 0),
            0xED => {
                let width = if prefix.operand_size_override {
                    MemWidth::B2
                } else {
                    MemWidth::B4
                };
                (self.gpr(2), width, 0)
            }
            _ => {
                return Err(LiftError::InvalidEncoding {
                    addr: pc,
                    bytes: bytes.to_vec(),
                })
            }
        };

        let ops = vec![SmirOp::new(
            OpId(0),
            pc,
            OpKind::IoIn {
                dst: self.gpr(0),
                port,
                width,
            },
        )];

        Ok(LiftResult::fallthrough(ops, prefix.cursor + imm_len))
    }

    /// Lift OUT imm8 or DX (E6/E7/EE/EF)
    fn lift_out(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
    ) -> Result<LiftResult, LiftError> {
        let (port, width, imm_len) = match opcode {
            0xE6 => {
                if bytes.is_empty() {
                    return Err(LiftError::Incomplete {
                        addr: pc,
                        have: 0,
                        need: 1,
                    });
                }
                (VReg::Imm(bytes[0] as i8 as i64), MemWidth::B1, 1)
            }
            0xE7 => {
                if bytes.is_empty() {
                    return Err(LiftError::Incomplete {
                        addr: pc,
                        have: 0,
                        need: 1,
                    });
                }
                let width = if prefix.operand_size_override {
                    MemWidth::B2
                } else {
                    MemWidth::B4
                };
                (VReg::Imm(bytes[0] as i8 as i64), width, 1)
            }
            0xEE => (self.gpr(2), MemWidth::B1, 0),
            0xEF => {
                let width = if prefix.operand_size_override {
                    MemWidth::B2
                } else {
                    MemWidth::B4
                };
                (self.gpr(2), width, 0)
            }
            _ => {
                return Err(LiftError::InvalidEncoding {
                    addr: pc,
                    bytes: bytes.to_vec(),
                })
            }
        };

        let ops = vec![SmirOp::new(
            OpId(0),
            pc,
            OpKind::IoOut {
                port,
                value: self.gpr(0),
                width,
            },
        )];

        Ok(LiftResult::fallthrough(ops, prefix.cursor + imm_len))
    }

    /// Lift NOP (90)
    fn lift_nop(&self, prefix: &X86Prefix, _pc: u64) -> Result<LiftResult, LiftError> {
        Ok(LiftResult::fallthrough(vec![], prefix.cursor))
    }

    /// Lift MOV r/m, r and MOV r, r/m (88-8B)
    fn lift_mov_rm_r(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let is_8bit = (opcode & 0x01) == 0;
        let dir_reg_rm = (opcode & 0x02) != 0; // true = reg is src, rm is dst

        let op_size = if is_8bit { 1 } else { prefix.op_size() };
        let width = self.size_to_width(op_size);
        let mem_width = self.size_to_memwidth(op_size);

        let modrm = decode_modrm(bytes, prefix, pc)?;
        let mut ops = Vec::new();
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;

        if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            if dir_reg_rm {
                // MOV r, rm - load from memory
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Load {
                        dst: self.gpr(modrm.reg),
                        addr,
                        width: mem_width,
                        sign: SignExtend::Zero,
                    },
                ));
            } else {
                // MOV rm, r - store to memory
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Store {
                        src: self.gpr(modrm.reg),
                        addr,
                        width: mem_width,
                    },
                ));
            }
        } else {
            // Register to register
            let (dst, src) = if dir_reg_rm {
                (self.gpr(modrm.reg), self.gpr(modrm.rm))
            } else {
                (self.gpr(modrm.rm), self.gpr(modrm.reg))
            };

            ops.push(SmirOp::new(
                OpId(0),
                pc,
                OpKind::Mov {
                    dst,
                    src: SrcOperand::Reg(src),
                    width,
                },
            ));
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed,
        ))
    }

    /// Lift MOVSXD r64, r/m32 (63)
    fn lift_movsxd(
        &self,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let modrm = decode_modrm(bytes, prefix, pc)?;
        let mut ops = Vec::new();
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;

        if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Load {
                    dst: tmp,
                    addr,
                    width: MemWidth::B4,
                    sign: SignExtend::Sign,
                },
            ));
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::SignExtend {
                    dst: self.gpr(modrm.reg),
                    src: tmp,
                    from_width: OpWidth::W32,
                    to_width: OpWidth::W64,
                },
            ));
        } else {
            ops.push(SmirOp::new(
                OpId(0),
                pc,
                OpKind::SignExtend {
                    dst: self.gpr(modrm.reg),
                    src: self.gpr(modrm.rm),
                    from_width: OpWidth::W32,
                    to_width: OpWidth::W64,
                },
            ));
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed,
        ))
    }

    /// Lift MOV r/m, imm (C6/C7)
    fn lift_mov_rm_imm(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let is_8bit = opcode == 0xC6;
        let op_size = if is_8bit { 1 } else { prefix.op_size() };
        let width = self.size_to_width(op_size);
        let mem_width = self.size_to_memwidth(op_size);

        let modrm = decode_modrm(bytes, prefix, pc)?;
        let group = (modrm.byte >> 3) & 0x07;
        if group != 0 {
            if self.strict {
                return Err(LiftError::Unsupported {
                    addr: pc,
                    mnemonic: format!("mov group {}", group),
                });
            }
            return Ok(LiftResult::fallthrough(
                vec![SmirOp::new(OpId(0), pc, OpKind::Nop)],
                prefix.cursor + modrm.bytes_consumed,
            ));
        }

        let imm_offset = modrm.bytes_consumed;
        let imm_size = if is_8bit {
            1
        } else if op_size == 2 {
            2
        } else {
            4
        };
        if bytes.len() < imm_offset + imm_size {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: bytes.len(),
                need: imm_offset + imm_size,
            });
        }

        let imm = match imm_size {
            1 => bytes[imm_offset] as i8 as i64,
            2 => i16::from_le_bytes([bytes[imm_offset], bytes[imm_offset + 1]]) as i64,
            _ => i32::from_le_bytes([
                bytes[imm_offset],
                bytes[imm_offset + 1],
                bytes[imm_offset + 2],
                bytes[imm_offset + 3],
            ]) as i64,
        };

        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64 + imm_size as u64;
        let mut ops = Vec::new();
        let hint = X86OpHint::MovImmModRm;

        if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            ops.push(SmirOp::with_hint(
                OpId(ops.len() as u16),
                pc,
                OpKind::Store {
                    src: VReg::Imm(imm),
                    addr,
                    width: mem_width,
                },
                hint,
            ));
        } else {
            ops.push(SmirOp::with_hint(
                OpId(ops.len() as u16),
                pc,
                OpKind::Mov {
                    dst: self.gpr(modrm.rm),
                    src: SrcOperand::Imm(imm),
                    width,
                },
                hint,
            ));
        }

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed + imm_size,
        ))
    }

    /// Lift TEST r/m, r (84/85)
    fn lift_test_rm_r(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let is_8bit = (opcode & 0x01) == 0;
        let op_size = if is_8bit { 1 } else { prefix.op_size() };
        let width = self.size_to_width(op_size);

        let modrm = decode_modrm(bytes, prefix, pc)?;
        let mut ops = Vec::new();
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;

        let (src1, src2) = if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            let tmp = ctx.alloc_vreg();
            ops.push(SmirOp::new(
                OpId(ops.len() as u16),
                pc,
                OpKind::Load {
                    dst: tmp,
                    addr,
                    width: self.size_to_memwidth(op_size),
                    sign: SignExtend::Zero,
                },
            ));
            (tmp, self.gpr(modrm.reg))
        } else {
            (self.gpr(modrm.rm), self.gpr(modrm.reg))
        };

        ops.push(SmirOp::new(
            OpId(ops.len() as u16),
            pc,
            OpKind::Test {
                src1,
                src2: SrcOperand::Reg(src2),
                width,
            },
        ));

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed,
        ))
    }

    /// Lift XOR r/m, r and XOR r, r/m (30-33)
    fn lift_xor_rm_r(
        &self,
        opcode: u8,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        let is_8bit = (opcode & 0x01) == 0;
        let dir_reg_rm = (opcode & 0x02) != 0;

        let op_size = if is_8bit { 1 } else { prefix.op_size() };
        let width = self.size_to_width(op_size);

        let modrm = decode_modrm(bytes, prefix, pc)?;
        let mut ops = Vec::new();
        let next_pc = pc + prefix.cursor as u64 + modrm.bytes_consumed as u64;
        let hint = X86OpHint::AluEncoding(if dir_reg_rm {
            X86AluEncoding::RegRm
        } else {
            X86AluEncoding::RmReg
        });

        let (dst, src1, src2) = if modrm.is_memory {
            let x86_addr = modrm.addr.as_ref().unwrap();
            let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
            ops.extend(pre_ops);

            if dir_reg_rm {
                // XOR r, rm
                let tmp = ctx.alloc_vreg();
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Load {
                        dst: tmp,
                        addr,
                        width: self.size_to_memwidth(op_size),
                        sign: SignExtend::Zero,
                    },
                ));
                (self.gpr(modrm.reg), self.gpr(modrm.reg), tmp)
            } else {
                // XOR rm, r - load-modify-store
                let tmp = ctx.alloc_vreg();
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Load {
                        dst: tmp,
                        addr: addr.clone(),
                        width: self.size_to_memwidth(op_size),
                        sign: SignExtend::Zero,
                    },
                ));
                ops.push(SmirOp::with_hint(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Xor {
                        dst: tmp,
                        src1: tmp,
                        src2: SrcOperand::Reg(self.gpr(modrm.reg)),
                        width,
                        flags: FlagUpdate::All,
                    },
                    hint,
                ));
                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::Store {
                        src: tmp,
                        addr,
                        width: self.size_to_memwidth(op_size),
                    },
                ));
                return Ok(LiftResult::fallthrough(
                    ops,
                    prefix.cursor + modrm.bytes_consumed,
                ));
            }
        } else if dir_reg_rm {
            (self.gpr(modrm.reg), self.gpr(modrm.reg), self.gpr(modrm.rm))
        } else {
            (self.gpr(modrm.rm), self.gpr(modrm.rm), self.gpr(modrm.reg))
        };

        let result = dst;
        ops.push(SmirOp::with_hint(
            OpId(ops.len() as u16),
            pc,
            OpKind::Xor {
                dst: result,
                src1,
                src2: SrcOperand::Reg(src2),
                width,
                flags: FlagUpdate::All,
            },
            hint,
        ));

        Ok(LiftResult::fallthrough(
            ops,
            prefix.cursor + modrm.bytes_consumed,
        ))
    }

    /// Lift the main instruction
    fn lift_insn_inner(
        &self,
        pc: u64,
        bytes: &[u8],
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        if bytes.is_empty() {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: 0,
                need: 1,
            });
        }

        // Decode prefixes
        let prefix = decode_prefixes(bytes)?;
        let opcode_bytes = &bytes[prefix.cursor..];

        if opcode_bytes.is_empty() {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: bytes.len(),
                need: prefix.cursor + 1,
            });
        }

        let opcode = opcode_bytes[0];
        let after_opcode = &opcode_bytes[1..];

        match opcode {
            // XCHG rax, r64 / NOP / PAUSE (with REP prefix)
            0x90..=0x97 => {
                if opcode == 0x90 && prefix.rep_prefix == Some(0xF3) {
                    // PAUSE - treat as NOP for lifting
                    Ok(LiftResult::fallthrough(vec![], prefix.cursor + 1))
                } else {
                    self.lift_xchg_rax(
                        opcode,
                        &X86Prefix {
                            cursor: prefix.cursor + 1,
                            ..prefix
                        },
                        pc,
                    )
                }
            }

            // CMC/CLC/STC
            0xF5 => Ok(LiftResult::fallthrough(
                vec![SmirOp::new(OpId(0), pc, OpKind::CmcCF)],
                prefix.cursor + 1,
            )),
            0xF8 => Ok(LiftResult::fallthrough(
                vec![SmirOp::new(OpId(0), pc, OpKind::SetCF { value: false })],
                prefix.cursor + 1,
            )),
            0xF9 => Ok(LiftResult::fallthrough(
                vec![SmirOp::new(OpId(0), pc, OpKind::SetCF { value: true })],
                prefix.cursor + 1,
            )),
            0xFC => Ok(LiftResult::fallthrough(
                vec![SmirOp::new(OpId(0), pc, OpKind::SetDF { value: false })],
                prefix.cursor + 1,
            )),
            0xFD => Ok(LiftResult::fallthrough(
                vec![SmirOp::new(OpId(0), pc, OpKind::SetDF { value: true })],
                prefix.cursor + 1,
            )),
            0xCC => Ok(LiftResult::fallthrough(
                vec![SmirOp::new(OpId(0), pc, OpKind::Breakpoint)],
                prefix.cursor + 1,
            )),

            // HLT
            0xF4 => Ok(LiftResult {
                ops: vec![],
                bytes_consumed: prefix.cursor + 1,
                control_flow: ControlFlow::Trap {
                    kind: TrapKind::Halt,
                },
                branch_targets: vec![],
            }),

            // Two-byte opcode prefix
            0x0F => self.lift_0f_opcode(after_opcode, &prefix, pc, ctx),

            // Control flow
            0xEB => self.lift_jmp_rel8(
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0xE9 => self.lift_jmp_rel32(
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0xE8 => self.lift_call_rel32(
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0xC2 => self.lift_ret_imm16(
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0xC3 => self.lift_ret(
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0xC9 => self.lift_leave(
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
            ),
            0x99 => self.lift_cwd_cdq_cqo(
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
            ),
            0x70..=0x7F => self.lift_jcc_rel8(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),

            // Data movement
            0xB0..=0xB7 => self.lift_mov_r8_imm8(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0xB8..=0xBF => self.lift_mov_r_imm(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0x88..=0x8B => self.lift_mov_rm_r(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0xC6 | 0xC7 => self.lift_mov_rm_imm(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0x8D => self.lift_lea(
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0x63 => self.lift_movsxd(
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0x50..=0x57 => self.lift_push_r64(
                opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0x58..=0x5F => self.lift_pop_r64(
                opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0x6A | 0x68 => self.lift_push_imm(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
            ),
            0xF6 | 0xF7 => self.lift_group3(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0x69 | 0x6B => self.lift_imul_rmi(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),

            // Arithmetic
            0x00..=0x05 => self.lift_arith(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ), // ADD
            0x08..=0x0D => self.lift_arith(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ), // OR
            0x10..=0x15 => self.lift_arith(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ), // ADC
            0x18..=0x1D => self.lift_arith(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ), // SBB
            0x20..=0x25 => self.lift_arith(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ), // AND
            0x28..=0x2D => self.lift_arith(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ), // SUB
            0x30..=0x33 => self.lift_xor_rm_r(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0x38..=0x3D => self.lift_arith(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ), // CMP

            // Group 1 immediate (80/81/83)
            0x80 | 0x81 | 0x83 => self.lift_group1_imm(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),

            // Logic
            0x84 | 0x85 => self.lift_test_rm_r(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),

            // String ops
            0xAA | 0xAB => self.lift_stos(
                opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),
            0xA4 | 0xA5 => self.lift_movs(
                opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
            ),

            // Shift/rotate group (C0/C1) - immediate
            0xC0 | 0xC1 => self.lift_shift_imm(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),

            // Shift/rotate group (D0/D1) - count = 1
            0xD0 | 0xD1 => self.lift_shift_one(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),

            // Shift/rotate group (D2/D3) - count in CL
            0xD2 | 0xD3 => self.lift_shift_cl(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),

            // Group 5 (FF)
            0xFF => self.lift_group5(
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
                ctx,
            ),

            // I/O port instructions
            0xE4 | 0xE5 | 0xEC | 0xED => self.lift_in(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
            ),
            0xE6 | 0xE7 | 0xEE | 0xEF => self.lift_out(
                opcode,
                after_opcode,
                &X86Prefix {
                    cursor: prefix.cursor + 1,
                    ..prefix
                },
                pc,
            ),

            // Unsupported - return error with mnemonic
            _ => {
                if self.strict {
                    Err(LiftError::Unsupported {
                        addr: pc,
                        mnemonic: format!("0x{:02X}", opcode),
                    })
                } else {
                    // In non-strict mode, emit a Nop and continue
                    Ok(LiftResult::fallthrough(
                        vec![SmirOp::new(OpId(0), pc, OpKind::Nop)],
                        prefix.cursor + 1,
                    ))
                }
            }
        }
    }

    /// Lift 0F-prefixed (two-byte) opcodes
    fn lift_0f_opcode(
        &self,
        bytes: &[u8],
        prefix: &X86Prefix,
        pc: u64,
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        if bytes.is_empty() {
            return Err(LiftError::Incomplete {
                addr: pc,
                have: prefix.cursor + 1,
                need: prefix.cursor + 2,
            });
        }

        let opcode2 = bytes[0];
        let after_opcode = &bytes[1..];
        let prefix2 = X86Prefix {
            cursor: prefix.cursor + 2,
            ..prefix.clone()
        };

        match opcode2 {
            // Jcc rel32 (0F 80 - 0F 8F)
            0x80..=0x8F => {
                if after_opcode.len() < 4 {
                    return Err(LiftError::Incomplete {
                        addr: pc,
                        have: prefix.cursor + 2 + after_opcode.len(),
                        need: prefix.cursor + 6,
                    });
                }

                let cc = opcode2 & 0x0F;
                let cond = self.x86_cond(cc);
                let rel = i32::from_le_bytes([
                    after_opcode[0],
                    after_opcode[1],
                    after_opcode[2],
                    after_opcode[3],
                ]) as i64;

                let insn_len = prefix.cursor + 6;
                let next_pc = pc + insn_len as u64;
                let target = (next_pc as i64 + rel) as u64;

                Ok(LiftResult::cond_branch(
                    vec![],
                    insn_len,
                    cond,
                    target,
                    next_pc,
                ))
            }

            // SETcc (0F 90 - 0F 9F)
            0x90..=0x9F => {
                let cc = opcode2 & 0x0F;
                let cond = self.x86_cond(cc);

                let modrm = decode_modrm(after_opcode, &prefix2, pc)?;
                let mut ops = Vec::new();
                let next_pc = pc + prefix2.cursor as u64 + modrm.bytes_consumed as u64;

                if modrm.is_memory {
                    let x86_addr = modrm.addr.as_ref().unwrap();
                    let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                    ops.extend(pre_ops);

                    let tmp = ctx.alloc_vreg();
                    ops.push(SmirOp::new(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::SetCC {
                            dst: tmp,
                            cond,
                            width: OpWidth::W8,
                        },
                    ));
                    ops.push(SmirOp::new(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::Store {
                            src: tmp,
                            addr,
                            width: MemWidth::B1,
                        },
                    ));
                } else {
                    ops.push(SmirOp::new(
                        OpId(0),
                        pc,
                        OpKind::SetCC {
                            dst: self.gpr(modrm.rm),
                            cond,
                            width: OpWidth::W8,
                        },
                    ));
                }

                Ok(LiftResult::fallthrough(
                    ops,
                    prefix2.cursor + modrm.bytes_consumed,
                ))
            }

            // CMOVcc (0F 40 - 0F 4F)
            0x40..=0x4F => {
                let cc = opcode2 & 0x0F;
                let cond = self.x86_cond(cc);
                let op_size = prefix.op_size();
                let width = self.size_to_width(op_size);

                let modrm = decode_modrm(after_opcode, &prefix2, pc)?;
                let mut ops = Vec::new();
                let next_pc = pc + prefix2.cursor as u64 + modrm.bytes_consumed as u64;

                let src = if modrm.is_memory {
                    let x86_addr = modrm.addr.as_ref().unwrap();
                    let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                    ops.extend(pre_ops);

                    let tmp = ctx.alloc_vreg();
                    ops.push(SmirOp::new(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::Load {
                            dst: tmp,
                            addr,
                            width: self.size_to_memwidth(op_size),
                            sign: SignExtend::Zero,
                        },
                    ));
                    tmp
                } else {
                    self.gpr(modrm.rm)
                };

                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::CMove {
                        dst: self.gpr(modrm.reg),
                        src,
                        cond,
                        width,
                    },
                ));

                Ok(LiftResult::fallthrough(
                    ops,
                    prefix2.cursor + modrm.bytes_consumed,
                ))
            }

            // SSE MOVDQA/MOVDQU (0F 6F/7F)
            0x6F | 0x7F => self.lift_sse_mov(opcode2, after_opcode, &prefix2, pc, ctx),

            // SSE PADDD (66 0F FE)
            0xFE => self.lift_sse_padd(opcode2, after_opcode, &prefix2, pc, ctx),

            // SSE4.1 opcodes (0F 38)
            0x38 => self.lift_0f38_opcode(after_opcode, &prefix2, pc, ctx),

            // SHLD/SHRD (0F A4/A5/AC/AD)
            0xA4 | 0xA5 | 0xAC | 0xAD => {
                self.lift_shld_shrd(opcode2, after_opcode, &prefix2, pc, ctx)
            }

            // MOVZX r, r/m8 (0F B6)
            0xB6 => {
                let op_size = prefix.op_size();
                let modrm = decode_modrm(after_opcode, &prefix2, pc)?;
                let mut ops = Vec::new();
                let next_pc = pc + prefix2.cursor as u64 + modrm.bytes_consumed as u64;

                let src = if modrm.is_memory {
                    let x86_addr = modrm.addr.as_ref().unwrap();
                    let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                    ops.extend(pre_ops);

                    let tmp = ctx.alloc_vreg();
                    ops.push(SmirOp::new(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::Load {
                            dst: tmp,
                            addr,
                            width: MemWidth::B1,
                            sign: SignExtend::Zero,
                        },
                    ));
                    tmp
                } else {
                    self.gpr(modrm.rm)
                };

                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::ZeroExtend {
                        dst: self.gpr(modrm.reg),
                        src,
                        from_width: OpWidth::W8,
                        to_width: self.size_to_width(op_size),
                    },
                ));

                Ok(LiftResult::fallthrough(
                    ops,
                    prefix2.cursor + modrm.bytes_consumed,
                ))
            }

            // MOVZX r, r/m16 (0F B7)
            0xB7 => {
                let op_size = prefix.op_size();
                let modrm = decode_modrm(after_opcode, &prefix2, pc)?;
                let mut ops = Vec::new();
                let next_pc = pc + prefix2.cursor as u64 + modrm.bytes_consumed as u64;

                let src = if modrm.is_memory {
                    let x86_addr = modrm.addr.as_ref().unwrap();
                    let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                    ops.extend(pre_ops);

                    let tmp = ctx.alloc_vreg();
                    ops.push(SmirOp::new(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::Load {
                            dst: tmp,
                            addr,
                            width: MemWidth::B2,
                            sign: SignExtend::Zero,
                        },
                    ));
                    tmp
                } else {
                    self.gpr(modrm.rm)
                };

                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::ZeroExtend {
                        dst: self.gpr(modrm.reg),
                        src,
                        from_width: OpWidth::W16,
                        to_width: self.size_to_width(op_size),
                    },
                ));

                Ok(LiftResult::fallthrough(
                    ops,
                    prefix2.cursor + modrm.bytes_consumed,
                ))
            }

            // BSF/BSR (0F BC/0F BD)
            0xBC | 0xBD => self.lift_bsf_bsr(opcode2, after_opcode, &prefix2, pc, ctx),

            // MOVSX r, r/m8 (0F BE)
            0xBE => {
                let op_size = prefix.op_size();
                let modrm = decode_modrm(after_opcode, &prefix2, pc)?;
                let mut ops = Vec::new();
                let next_pc = pc + prefix2.cursor as u64 + modrm.bytes_consumed as u64;

                let src = if modrm.is_memory {
                    let x86_addr = modrm.addr.as_ref().unwrap();
                    let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                    ops.extend(pre_ops);

                    let tmp = ctx.alloc_vreg();
                    ops.push(SmirOp::new(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::Load {
                            dst: tmp,
                            addr,
                            width: MemWidth::B1,
                            sign: SignExtend::Sign,
                        },
                    ));
                    tmp
                } else {
                    self.gpr(modrm.rm)
                };

                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::SignExtend {
                        dst: self.gpr(modrm.reg),
                        src,
                        from_width: OpWidth::W8,
                        to_width: self.size_to_width(op_size),
                    },
                ));

                Ok(LiftResult::fallthrough(
                    ops,
                    prefix2.cursor + modrm.bytes_consumed,
                ))
            }

            // MOVSX r, r/m16 (0F BF)
            0xBF => {
                let op_size = prefix.op_size();
                let modrm = decode_modrm(after_opcode, &prefix2, pc)?;
                let mut ops = Vec::new();
                let next_pc = pc + prefix2.cursor as u64 + modrm.bytes_consumed as u64;

                let src = if modrm.is_memory {
                    let x86_addr = modrm.addr.as_ref().unwrap();
                    let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                    ops.extend(pre_ops);

                    let tmp = ctx.alloc_vreg();
                    ops.push(SmirOp::new(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::Load {
                            dst: tmp,
                            addr,
                            width: MemWidth::B2,
                            sign: SignExtend::Sign,
                        },
                    ));
                    tmp
                } else {
                    self.gpr(modrm.rm)
                };

                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::SignExtend {
                        dst: self.gpr(modrm.reg),
                        src,
                        from_width: OpWidth::W16,
                        to_width: self.size_to_width(op_size),
                    },
                ));

                Ok(LiftResult::fallthrough(
                    ops,
                    prefix2.cursor + modrm.bytes_consumed,
                ))
            }

            // IMUL r, r/m (0F AF)
            0xAF => {
                let op_size = prefix.op_size();
                let width = self.size_to_width(op_size);
                let modrm = decode_modrm(after_opcode, &prefix2, pc)?;
                let mut ops = Vec::new();
                let next_pc = pc + prefix2.cursor as u64 + modrm.bytes_consumed as u64;

                let src = if modrm.is_memory {
                    let x86_addr = modrm.addr.as_ref().unwrap();
                    let (addr, pre_ops) = self.x86_addr_to_smir(x86_addr, next_pc, ctx);
                    ops.extend(pre_ops);

                    let tmp = ctx.alloc_vreg();
                    ops.push(SmirOp::new(
                        OpId(ops.len() as u16),
                        pc,
                        OpKind::Load {
                            dst: tmp,
                            addr,
                            width: self.size_to_memwidth(op_size),
                            sign: SignExtend::Zero,
                        },
                    ));
                    tmp
                } else {
                    self.gpr(modrm.rm)
                };

                ops.push(SmirOp::new(
                    OpId(ops.len() as u16),
                    pc,
                    OpKind::MulS {
                        dst_lo: self.gpr(modrm.reg),
                        dst_hi: None,
                        src1: self.gpr(modrm.reg),
                        src2: SrcOperand::Reg(src),
                        width,
                        flags: FlagUpdate::All,
                    },
                ));

                Ok(LiftResult::fallthrough(
                    ops,
                    prefix2.cursor + modrm.bytes_consumed,
                ))
            }

            // SYSCALL (0F 05)
            0x05 => Ok(LiftResult {
                ops: vec![],
                bytes_consumed: prefix.cursor + 2,
                control_flow: ControlFlow::Syscall,
                branch_targets: vec![],
            }),

            // SYSRET (0F 07)
            0x07 => {
                // Treat as return for lifting purposes
                Ok(LiftResult::ret(vec![], prefix.cursor + 2))
            }

            // NOP (0F 1F /0) - multi-byte NOP
            0x1F => {
                let modrm = decode_modrm(after_opcode, &prefix2, pc)?;
                Ok(LiftResult::fallthrough(
                    vec![],
                    prefix2.cursor + modrm.bytes_consumed,
                ))
            }

            _ => {
                if self.strict {
                    Err(LiftError::Unsupported {
                        addr: pc,
                        mnemonic: format!("0x0F 0x{:02X}", opcode2),
                    })
                } else {
                    Ok(LiftResult::fallthrough(
                        vec![SmirOp::new(OpId(0), pc, OpKind::Nop)],
                        prefix.cursor + 2,
                    ))
                }
            }
        }
    }
}

// ============================================================================
// SmirLifter Implementation
// ============================================================================

impl SmirLifter for X86_64Lifter {
    fn source_arch(&self) -> SourceArch {
        SourceArch::X86_64
    }

    fn lift_insn(
        &mut self,
        addr: GuestAddr,
        bytes: &[u8],
        ctx: &mut LiftContext,
    ) -> Result<LiftResult, LiftError> {
        ctx.guest_pc = addr;
        self.lift_insn_inner(addr, bytes, ctx)
    }

    fn lift_block(
        &mut self,
        addr: GuestAddr,
        mem: &dyn MemoryReader,
        ctx: &mut LiftContext,
    ) -> Result<SmirBlock, LiftError> {
        let block_id = ctx.get_or_create_block(addr);
        let mut block = SmirBlock::new(block_id, addr);

        let mut pc = addr;
        let mut buf = [0u8; 15];

        loop {
            // Read instruction bytes
            let bytes = mem
                .read(pc, 15)
                .map_err(|e| LiftError::MemoryError { addr: pc, error: e })?;

            buf[..bytes.len()].copy_from_slice(&bytes);

            ctx.guest_pc = pc;
            let result = self.lift_insn_inner(pc, &buf[..bytes.len()], ctx)?;

            // Add ops to block
            block.ops.extend(result.ops);
            pc += result.bytes_consumed as u64;

            // Check for block-ending control flow
            match result.control_flow {
                ControlFlow::Fallthrough | ControlFlow::NextInsn => continue,
                ControlFlow::Branch { target } | ControlFlow::DirectBranch(target) => {
                    block.terminator = Terminator::Branch {
                        target: ctx.get_or_create_block(target),
                    };
                    break;
                }
                ControlFlow::CondBranch {
                    cond,
                    target,
                    fallthrough,
                } => {
                    // We need a VReg holding the condition result
                    let cond_vreg = ctx.alloc_vreg();
                    block.ops.push(SmirOp::new(
                        OpId(block.ops.len() as u16),
                        pc,
                        OpKind::TestCondition {
                            dst: cond_vreg,
                            cond,
                        },
                    ));
                    block.terminator = Terminator::CondBranch {
                        cond: cond_vreg,
                        true_target: ctx.get_or_create_block(target),
                        false_target: ctx.get_or_create_block(fallthrough),
                    };
                    break;
                }
                ControlFlow::CondBranchReg {
                    cond,
                    taken,
                    not_taken,
                } => {
                    block.terminator = Terminator::CondBranch {
                        cond,
                        true_target: ctx.get_or_create_block(taken),
                        false_target: ctx.get_or_create_block(not_taken),
                    };
                    break;
                }
                ControlFlow::IndirectBranch { target } => {
                    block.terminator = Terminator::IndirectBranch {
                        target,
                        possible_targets: vec![],
                    };
                    break;
                }
                ControlFlow::IndirectBranchMem { addr } => {
                    block.terminator = Terminator::IndirectBranchMem {
                        addr,
                        possible_targets: vec![],
                    };
                    break;
                }
                ControlFlow::Call { target } => {
                    let continuation = ctx.get_or_create_block(pc);
                    block.terminator = Terminator::Call {
                        target,
                        args: vec![],
                        continuation,
                    };
                    break;
                }
                ControlFlow::Return => {
                    block.terminator = Terminator::Return { values: vec![] };
                    break;
                }
                ControlFlow::Trap { kind } => {
                    block.terminator = Terminator::Trap { kind };
                    break;
                }
                ControlFlow::Syscall => {
                    // For syscall, we'll use a TailCall to the syscall runtime
                    block.terminator = Terminator::TailCall {
                        target: CallTarget::Runtime(crate::smir::ir::RuntimeFunc::Syscall),
                        args: vec![],
                    };
                    break;
                }
            }
        }

        Ok(block)
    }

    fn lift_function(
        &mut self,
        entry: GuestAddr,
        mem: &dyn MemoryReader,
        ctx: &mut LiftContext,
    ) -> Result<SmirFunction, LiftError> {
        let entry_block = ctx.get_or_create_block(entry);
        let mut func = SmirFunction::new(FunctionId(entry as u32), entry_block, entry);

        // Work queue of blocks to lift
        let mut worklist = vec![entry];
        let mut visited = HashSet::new();

        while let Some(block_addr) = worklist.pop() {
            if visited.contains(&block_addr) {
                continue;
            }
            visited.insert(block_addr);

            let block = self.lift_block(block_addr, mem, ctx)?;

            // Add branch targets to worklist
            match &block.terminator {
                Terminator::Branch { target } => {
                    if let Some(&addr) =
                        ctx.block_cache.iter().find_map(
                            |(a, id)| {
                                if id == target {
                                    Some(a)
                                } else {
                                    None
                                }
                            },
                        )
                    {
                        worklist.push(addr);
                    }
                }
                Terminator::CondBranch {
                    true_target,
                    false_target,
                    ..
                } => {
                    for target in [true_target, false_target] {
                        if let Some(&addr) = ctx.block_cache.iter().find_map(|(a, id)| {
                            if id == target {
                                Some(a)
                            } else {
                                None
                            }
                        }) {
                            worklist.push(addr);
                        }
                    }
                }
                _ => {}
            }

            func.add_block(block);
        }

        Ok(func)
    }
}

// ============================================================================
// Tests
// ============================================================================

#[cfg(test)]
mod tests {
    use super::*;

    /// Test memory reader for unit tests
    struct TestMemory {
        data: Vec<u8>,
        base: u64,
    }

    impl TestMemory {
        fn new(base: u64, data: Vec<u8>) -> Self {
            TestMemory { data, base }
        }
    }

    impl MemoryReader for TestMemory {
        fn read(&self, addr: u64, size: usize) -> Result<Vec<u8>, MemoryError> {
            if addr < self.base {
                return Err(MemoryError::OutOfBounds { addr });
            }
            let offset = (addr - self.base) as usize;
            if offset >= self.data.len() {
                return Err(MemoryError::OutOfBounds { addr });
            }
            // Return as many bytes as possible up to size
            let available = (self.data.len() - offset).min(size);
            Ok(self.data[offset..offset + available].to_vec())
        }
    }

    #[test]
    fn test_prefix_decode() {
        // No prefix
        let prefix = decode_prefixes(&[0x90]).unwrap();
        assert_eq!(prefix.cursor, 0);
        assert!(!prefix.has_rex());

        // REX.W prefix
        let prefix = decode_prefixes(&[0x48, 0xB8]).unwrap();
        assert_eq!(prefix.cursor, 1);
        assert!(prefix.rex_w());
        assert_eq!(prefix.op_size(), 8);

        // Operand size override
        let prefix = decode_prefixes(&[0x66, 0xB8]).unwrap();
        assert_eq!(prefix.cursor, 1);
        assert!(prefix.operand_size_override);
        assert_eq!(prefix.op_size(), 2);
    }

    #[test]
    fn test_modrm_decode() {
        // MOD=3 (register)
        let prefix = X86Prefix::default();
        let modrm = decode_modrm(&[0xC0], &prefix, 0).unwrap();
        assert!(!modrm.is_memory);
        assert_eq!(modrm.reg, 0);
        assert_eq!(modrm.rm, 0);

        // MOD=0, RM=5 (RIP-relative)
        let modrm = decode_modrm(&[0x05, 0x10, 0x00, 0x00, 0x00], &prefix, 0).unwrap();
        assert!(modrm.is_memory);
        assert!(modrm.addr.as_ref().unwrap().rip_relative);
        assert_eq!(modrm.bytes_consumed, 5);
    }

    #[test]
    fn test_lift_nop() {
        let mut lifter = X86_64Lifter::new();
        let mut ctx = LiftContext::new(SourceArch::X86_64);

        // NOP
        let result = lifter.lift_insn(0x1000, &[0x90], &mut ctx).unwrap();
        assert_eq!(result.bytes_consumed, 1);
        assert!(matches!(result.control_flow, ControlFlow::Fallthrough));
    }

    #[test]
    fn test_lift_mov_r_imm() {
        let mut lifter = X86_64Lifter::new();
        let mut ctx = LiftContext::new(SourceArch::X86_64);

        // MOV EAX, 0x12345678
        let result = lifter
            .lift_insn(0x1000, &[0xB8, 0x78, 0x56, 0x34, 0x12], &mut ctx)
            .unwrap();
        assert_eq!(result.bytes_consumed, 5);
        assert_eq!(result.ops.len(), 1);

        // MOV RAX, 0x123456789ABCDEF0 (REX.W prefix)
        let result = lifter
            .lift_insn(
                0x1000,
                &[0x48, 0xB8, 0xF0, 0xDE, 0xBC, 0x9A, 0x78, 0x56, 0x34, 0x12],
                &mut ctx,
            )
            .unwrap();
        assert_eq!(result.bytes_consumed, 10);
    }

    #[test]
    fn test_lift_jmp() {
        let mut lifter = X86_64Lifter::new();
        let mut ctx = LiftContext::new(SourceArch::X86_64);

        // JMP rel8
        let result = lifter.lift_insn(0x1000, &[0xEB, 0x10], &mut ctx).unwrap();
        assert_eq!(result.bytes_consumed, 2);
        assert!(matches!(
            result.control_flow,
            ControlFlow::Branch { target: 0x1012 }
        ));

        // JMP rel32
        let result = lifter
            .lift_insn(0x1000, &[0xE9, 0x00, 0x10, 0x00, 0x00], &mut ctx)
            .unwrap();
        assert_eq!(result.bytes_consumed, 5);
        assert!(matches!(
            result.control_flow,
            ControlFlow::Branch { target: 0x2005 }
        ));
    }

    #[test]
    fn test_lift_jcc() {
        let mut lifter = X86_64Lifter::new();
        let mut ctx = LiftContext::new(SourceArch::X86_64);

        // JE rel8
        let result = lifter.lift_insn(0x1000, &[0x74, 0x10], &mut ctx).unwrap();
        assert_eq!(result.bytes_consumed, 2);
        match result.control_flow {
            ControlFlow::CondBranch {
                cond,
                target,
                fallthrough,
            } => {
                assert_eq!(cond, Condition::Eq);
                assert_eq!(target, 0x1012);
                assert_eq!(fallthrough, 0x1002);
            }
            _ => panic!("Expected CondBranch"),
        }
    }

    #[test]
    fn test_lift_push_pop() {
        let mut lifter = X86_64Lifter::new();
        let mut ctx = LiftContext::new(SourceArch::X86_64);

        // PUSH RAX
        let result = lifter.lift_insn(0x1000, &[0x50], &mut ctx).unwrap();
        assert_eq!(result.bytes_consumed, 1);
        assert_eq!(result.ops.len(), 2); // SUB RSP + STORE

        // POP RAX
        let result = lifter.lift_insn(0x1000, &[0x58], &mut ctx).unwrap();
        assert_eq!(result.bytes_consumed, 1);
        assert_eq!(result.ops.len(), 2); // LOAD + ADD RSP
    }

    #[test]
    fn test_lift_call_ret() {
        let mut lifter = X86_64Lifter::new();
        let mut ctx = LiftContext::new(SourceArch::X86_64);

        // CALL rel32
        let result = lifter
            .lift_insn(0x1000, &[0xE8, 0x00, 0x10, 0x00, 0x00], &mut ctx)
            .unwrap();
        assert_eq!(result.bytes_consumed, 5);
        assert!(matches!(
            result.control_flow,
            ControlFlow::Call {
                target: CallTarget::GuestAddr(0x2005)
            }
        ));

        // RET
        let result = lifter.lift_insn(0x1000, &[0xC3], &mut ctx).unwrap();
        assert_eq!(result.bytes_consumed, 1);
        assert!(matches!(result.control_flow, ControlFlow::Return));
    }

    #[test]
    fn test_lift_block() {
        let mut lifter = X86_64Lifter::new();
        let mut ctx = LiftContext::new(SourceArch::X86_64);

        // Simple block: MOV EAX, 1; RET
        let mem = TestMemory::new(0x1000, vec![0xB8, 0x01, 0x00, 0x00, 0x00, 0xC3]);
        let block = lifter.lift_block(0x1000, &mem, &mut ctx).unwrap();

        assert_eq!(block.guest_pc, 0x1000);
        assert!(matches!(block.terminator, Terminator::Return { .. }));
    }
}
