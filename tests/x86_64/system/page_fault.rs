//! Comprehensive tests for x86-64 page fault handling.
//!
//! Tests cover:
//! - Non-present page faults (read, write, execute)
//! - Write to read-only page
//! - User-mode access to supervisor page
//! - Page fault error codes (P, W/R, U/S, RSVD, I/D bits)
//! - CR2 register (faulting address)
//! - Page table walk with 4KB, 2MB, and 1GB pages
//! - Double fault (page fault during page fault delivery)
//! - Reserved bit violations

use std::sync::Arc;
use vm_memory::{Bytes, GuestAddress, GuestMemoryMmap};

use rax::backend::emulator::x86_64::X86_64Vcpu;
use rax::cpu::{Registers, SystemRegisters, Segment, DescriptorTable, VCpu, VcpuExit};

/// Physical addresses for page table structures
const PML4_ADDR: u64 = 0x1000;
const PDPT_ADDR: u64 = 0x2000;
const PD_ADDR: u64 = 0x3000;
const PT_ADDR: u64 = 0x4000;

/// Code and data addresses (virtual)
const CODE_VADDR: u64 = 0x10000;
const CODE_PADDR: u64 = 0x10000;  // Identity mapped for simplicity
const STACK_VADDR: u64 = 0x20000;
const STACK_PADDR: u64 = 0x20000;
const DATA_VADDR: u64 = 0x30000;
const DATA_PADDR: u64 = 0x30000;

/// IDT and handler addresses
const IDT_ADDR: u64 = 0x5000;
const PF_HANDLER_ADDR: u64 = 0x6000;  // Page fault handler
const DF_HANDLER_ADDR: u64 = 0x7000;  // Double fault handler
const GP_HANDLER_ADDR: u64 = 0x8000;  // General protection handler

/// Test result storage (physical address)
const RESULT_ADDR: u64 = 0x40000;

/// Page table entry flags
mod pte_flags {
    pub const PRESENT: u64 = 1 << 0;
    pub const WRITABLE: u64 = 1 << 1;
    pub const USER: u64 = 1 << 2;
    pub const ACCESSED: u64 = 1 << 5;
    pub const DIRTY: u64 = 1 << 6;
    pub const HUGE_PAGE: u64 = 1 << 7;
    pub const NO_EXECUTE: u64 = 1 << 63;
}

/// Page fault error code bits
mod pf_error {
    pub const P: u64 = 1 << 0;      // 0=non-present, 1=protection violation
    pub const WR: u64 = 1 << 1;     // 0=read, 1=write
    pub const US: u64 = 1 << 2;     // 0=supervisor, 1=user
    pub const RSVD: u64 = 1 << 3;   // Reserved bit violation
    pub const ID: u64 = 1 << 4;     // 0=data, 1=instruction fetch
}

/// Create guest memory with page tables and handlers
fn setup_paged_memory() -> Arc<GuestMemoryMmap> {
    let mem_size = 1024 * 1024; // 1MB
    let regions = vec![(GuestAddress(0), mem_size)];
    let mem = Arc::new(GuestMemoryMmap::<()>::from_ranges(&regions).unwrap());

    // Set up 4-level page tables (identity mapping for low memory)
    setup_page_tables(&mem);

    // Set up IDT with exception handlers
    setup_idt(&mem);

    // Set up exception handlers
    setup_handlers(&mem);

    mem
}

/// Set up 4-level page tables with identity mapping
fn setup_page_tables(mem: &GuestMemoryMmap) {
    use pte_flags::*;

    // PML4 entry 0 -> PDPT
    let pml4e = PDPT_ADDR | PRESENT | WRITABLE | USER;
    mem.write_slice(&pml4e.to_le_bytes(), GuestAddress(PML4_ADDR)).unwrap();

    // PDPT entry 0 -> PD
    let pdpte = PD_ADDR | PRESENT | WRITABLE | USER;
    mem.write_slice(&pdpte.to_le_bytes(), GuestAddress(PDPT_ADDR)).unwrap();

    // PD entry 0 -> PT (4KB pages)
    let pde = PT_ADDR | PRESENT | WRITABLE | USER;
    mem.write_slice(&pde.to_le_bytes(), GuestAddress(PD_ADDR)).unwrap();

    // PT entries: identity map pages 0-255 (1MB)
    for i in 0..256u64 {
        let paddr = i * 0x1000;
        let pte = paddr | PRESENT | WRITABLE | USER;
        let pt_offset = i * 8;
        mem.write_slice(&pte.to_le_bytes(), GuestAddress(PT_ADDR + pt_offset)).unwrap();
    }
}

/// Set up IDT with exception handlers
fn setup_idt(mem: &GuestMemoryMmap) {
    // IDT entry format (16 bytes each in 64-bit mode):
    // Bytes 0-1: Offset 15:0
    // Bytes 2-3: Segment selector (0x08 for code segment)
    // Byte 4: IST (0)
    // Byte 5: Type (0x8E = present, DPL=0, interrupt gate)
    // Bytes 6-7: Offset 31:16
    // Bytes 8-11: Offset 63:32
    // Bytes 12-15: Reserved (0)

    fn write_idt_entry(mem: &GuestMemoryMmap, vector: u64, handler_addr: u64) {
        let offset = IDT_ADDR + vector * 16;
        let mut entry = [0u8; 16];

        // Offset 15:0
        entry[0] = (handler_addr & 0xFF) as u8;
        entry[1] = ((handler_addr >> 8) & 0xFF) as u8;
        // Segment selector (0x08)
        entry[2] = 0x08;
        entry[3] = 0x00;
        // IST = 0
        entry[4] = 0x00;
        // Type = 0x8E (present, DPL=0, 64-bit interrupt gate)
        entry[5] = 0x8E;
        // Offset 31:16
        entry[6] = ((handler_addr >> 16) & 0xFF) as u8;
        entry[7] = ((handler_addr >> 24) & 0xFF) as u8;
        // Offset 63:32
        entry[8] = ((handler_addr >> 32) & 0xFF) as u8;
        entry[9] = ((handler_addr >> 40) & 0xFF) as u8;
        entry[10] = ((handler_addr >> 48) & 0xFF) as u8;
        entry[11] = ((handler_addr >> 56) & 0xFF) as u8;
        // Reserved
        entry[12..16].fill(0);

        mem.write_slice(&entry, GuestAddress(offset)).unwrap();
    }

    // #DF (8) - Double Fault
    write_idt_entry(mem, 8, DF_HANDLER_ADDR);

    // #GP (13) - General Protection Fault
    write_idt_entry(mem, 13, GP_HANDLER_ADDR);

    // #PF (14) - Page Fault
    write_idt_entry(mem, 14, PF_HANDLER_ADDR);
}

/// Set up exception handlers that store exception info and HLT
fn setup_handlers(mem: &GuestMemoryMmap) {
    // Page fault handler:
    // - Store error code (on stack) at RESULT_ADDR
    // - Store CR2 at RESULT_ADDR + 8
    // - Store marker 0xPF at RESULT_ADDR + 16
    // - HLT
    //
    // Stack at entry: ... | error_code | RIP | CS | RFLAGS | RSP | SS
    // We need to read error_code from [RSP]
    let pf_handler: &[u8] = &[
        // mov rax, [rsp]        ; Get error code from stack
        0x48, 0x8b, 0x04, 0x24,
        // mov [RESULT_ADDR], rax
        0x48, 0xa3,
        (RESULT_ADDR & 0xFF) as u8,
        ((RESULT_ADDR >> 8) & 0xFF) as u8,
        ((RESULT_ADDR >> 16) & 0xFF) as u8,
        ((RESULT_ADDR >> 24) & 0xFF) as u8,
        ((RESULT_ADDR >> 32) & 0xFF) as u8,
        ((RESULT_ADDR >> 40) & 0xFF) as u8,
        ((RESULT_ADDR >> 48) & 0xFF) as u8,
        ((RESULT_ADDR >> 56) & 0xFF) as u8,
        // mov rax, cr2          ; Get faulting address
        0x0f, 0x20, 0xd0,
        // mov [RESULT_ADDR+8], rax
        0x48, 0xa3,
        ((RESULT_ADDR + 8) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 8) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 16) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 24) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 32) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 40) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 48) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 56) & 0xFF) as u8,
        // mov rax, 0x14         ; PF marker (0x14 = #PF vector)
        0x48, 0xc7, 0xc0, 0x14, 0x00, 0x00, 0x00,
        // mov [RESULT_ADDR+16], rax
        0x48, 0xa3,
        ((RESULT_ADDR + 16) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 8) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 16) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 24) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 32) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 40) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 48) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 56) & 0xFF) as u8,
        // hlt
        0xf4,
    ];
    mem.write_slice(pf_handler, GuestAddress(PF_HANDLER_ADDR)).unwrap();

    // Double fault handler - similar but with 0x08 marker
    let df_handler: &[u8] = &[
        // mov rax, 0x08         ; DF marker
        0x48, 0xc7, 0xc0, 0x08, 0x00, 0x00, 0x00,
        // mov [RESULT_ADDR+16], rax
        0x48, 0xa3,
        ((RESULT_ADDR + 16) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 8) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 16) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 24) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 32) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 40) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 48) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 56) & 0xFF) as u8,
        // hlt
        0xf4,
    ];
    mem.write_slice(df_handler, GuestAddress(DF_HANDLER_ADDR)).unwrap();

    // GP handler
    let gp_handler: &[u8] = &[
        // mov rax, 0x0D         ; GP marker
        0x48, 0xc7, 0xc0, 0x0D, 0x00, 0x00, 0x00,
        // mov [RESULT_ADDR+16], rax
        0x48, 0xa3,
        ((RESULT_ADDR + 16) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 8) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 16) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 24) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 32) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 40) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 48) & 0xFF) as u8,
        (((RESULT_ADDR + 16) >> 56) & 0xFF) as u8,
        // hlt
        0xf4,
    ];
    mem.write_slice(gp_handler, GuestAddress(GP_HANDLER_ADDR)).unwrap();
}

/// Create a vCPU with paging enabled
fn create_paged_vcpu(mem: Arc<GuestMemoryMmap>) -> X86_64Vcpu {
    let mut vcpu = X86_64Vcpu::new(0, mem);

    let mut sregs = SystemRegisters::default();
    // Enable protected mode and paging
    sregs.cr0 = 0x80050033; // PE + PG + other standard bits
    sregs.cr3 = PML4_ADDR;  // Page table root
    sregs.cr4 = 0x20;       // PAE (required for 4-level paging)
    sregs.efer = 0x500;     // LME + LMA (long mode)

    // Set up code segment for 64-bit mode
    sregs.cs = Segment {
        base: 0,
        limit: 0xFFFFFFFF,
        selector: 0x08,
        type_: 0x0B,  // Execute/Read, accessed
        present: true,
        dpl: 0,
        db: false,    // Must be 0 for 64-bit
        s: true,      // Code/Data segment
        l: true,      // 64-bit mode
        g: true,
        avl: false,
        unusable: false,
    };

    // Set up data segments
    let data_seg = Segment {
        base: 0,
        limit: 0xFFFFFFFF,
        selector: 0x10,
        type_: 0x03,  // Read/Write, accessed
        present: true,
        dpl: 0,
        db: true,
        s: true,
        l: false,
        g: true,
        avl: false,
        unusable: false,
    };
    sregs.ds = data_seg.clone();
    sregs.es = data_seg.clone();
    sregs.fs = data_seg.clone();
    sregs.gs = data_seg.clone();
    sregs.ss = data_seg;

    // Set up IDT
    sregs.idt = DescriptorTable {
        base: IDT_ADDR,
        limit: 256 * 16 - 1,  // 256 entries
    };

    // Set up GDT (minimal - just needs valid selectors)
    sregs.gdt = DescriptorTable {
        base: 0x9000,
        limit: 0x1F,
    };

    vcpu.set_sregs(&sregs).unwrap();

    // Set initial registers
    let mut regs = Registers::default();
    regs.rsp = STACK_PADDR + 0x1000;  // Stack grows down
    regs.rflags = 0x2;  // Reserved bit must be 1
    vcpu.set_regs(&regs).unwrap();

    vcpu
}

/// Run vCPU until HLT
fn run_until_hlt(vcpu: &mut X86_64Vcpu) -> Result<(), String> {
    for _ in 0..100 {
        match vcpu.run() {
            Ok(VcpuExit::Hlt) => return Ok(()),
            Ok(VcpuExit::Shutdown) => return Err("Shutdown (triple fault?)".to_string()),
            Ok(_) => continue,
            Err(e) => return Err(format!("vCPU error: {:?}", e)),
        }
    }
    Err("Exceeded iteration limit".to_string())
}

/// Read result from RESULT_ADDR
fn read_result(mem: &GuestMemoryMmap) -> (u64, u64, u64) {
    let mut buf = [0u8; 8];
    mem.read_slice(&mut buf, GuestAddress(RESULT_ADDR)).unwrap();
    let error_code = u64::from_le_bytes(buf);

    mem.read_slice(&mut buf, GuestAddress(RESULT_ADDR + 8)).unwrap();
    let cr2 = u64::from_le_bytes(buf);

    mem.read_slice(&mut buf, GuestAddress(RESULT_ADDR + 16)).unwrap();
    let marker = u64::from_le_bytes(buf);

    (error_code, cr2, marker)
}

// ============================================================================
// Test Cases
// ============================================================================

/// Test read from non-present page
#[test]
fn test_pf_read_non_present() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Clear PT entry for page at 0x80000 (index 128)
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();

    // Code: read from 0x80000, then HLT
    let code: &[u8] = &[
        // mov rax, [0x80000]
        0x48, 0x8b, 0x04, 0x25, 0x00, 0x00, 0x08, 0x00,
        // hlt (should not reach here)
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (error_code, cr2, marker) = read_result(&mem);

    assert_eq!(marker, 0x14, "Should trigger #PF (vector 14)");
    assert_eq!(cr2, 0x80000, "CR2 should contain faulting address");
    // Error code: not present (P=0), read access (W/R=0), supervisor (U/S=0)
    assert_eq!(error_code & pf_error::P, 0, "P bit should be 0 (non-present)");
    assert_eq!(error_code & pf_error::WR, 0, "W/R bit should be 0 (read)");
    assert_eq!(error_code & pf_error::US, 0, "U/S bit should be 0 (supervisor)");
}

/// Test write to non-present page
#[test]
fn test_pf_write_non_present() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Clear PT entry for page at 0x80000
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();

    // Code: write to 0x80000
    let code: &[u8] = &[
        // mov qword [0x80000], 0x12345678
        0x48, 0xc7, 0x04, 0x25, 0x00, 0x00, 0x08, 0x00, 0x78, 0x56, 0x34, 0x12,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (error_code, cr2, marker) = read_result(&mem);

    assert_eq!(marker, 0x14, "Should trigger #PF");
    assert_eq!(cr2, 0x80000, "CR2 should contain faulting address");
    assert_eq!(error_code & pf_error::P, 0, "P bit should be 0 (non-present)");
    assert_eq!(error_code & pf_error::WR, pf_error::WR, "W/R bit should be 1 (write)");
}

/// Test write to read-only page (protection violation)
#[test]
fn test_pf_write_to_readonly() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Make page at 0x80000 present but read-only
    let pte = 0x80000u64 | pte_flags::PRESENT | pte_flags::USER;  // No WRITABLE flag
    mem.write_slice(&pte.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();

    // Code: write to 0x80000
    let code: &[u8] = &[
        // mov qword [0x80000], 0x12345678
        0x48, 0xc7, 0x04, 0x25, 0x00, 0x00, 0x08, 0x00, 0x78, 0x56, 0x34, 0x12,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (error_code, cr2, marker) = read_result(&mem);

    assert_eq!(marker, 0x14, "Should trigger #PF");
    assert_eq!(cr2, 0x80000, "CR2 should contain faulting address");
    assert_eq!(error_code & pf_error::P, pf_error::P, "P bit should be 1 (protection violation)");
    assert_eq!(error_code & pf_error::WR, pf_error::WR, "W/R bit should be 1 (write)");
}

/// Test read from read-only page (should succeed)
#[test]
fn test_read_readonly_succeeds() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Make page at 0x80000 present but read-only, and put data there
    let pte = 0x80000u64 | pte_flags::PRESENT | pte_flags::USER;
    mem.write_slice(&pte.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();
    mem.write_slice(&0xDEADBEEFCAFEBABEu64.to_le_bytes(), GuestAddress(0x80000)).unwrap();

    // Code: read from 0x80000 into rax, store at RESULT_ADDR, hlt
    let code: &[u8] = &[
        // mov rax, [0x80000]
        0x48, 0x8b, 0x04, 0x25, 0x00, 0x00, 0x08, 0x00,
        // mov [RESULT_ADDR], rax
        0x48, 0xa3,
        (RESULT_ADDR & 0xFF) as u8,
        ((RESULT_ADDR >> 8) & 0xFF) as u8,
        ((RESULT_ADDR >> 16) & 0xFF) as u8,
        ((RESULT_ADDR >> 24) & 0xFF) as u8,
        ((RESULT_ADDR >> 32) & 0xFF) as u8,
        ((RESULT_ADDR >> 40) & 0xFF) as u8,
        ((RESULT_ADDR >> 48) & 0xFF) as u8,
        ((RESULT_ADDR >> 56) & 0xFF) as u8,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    // Read result - should be the value we wrote
    let mut buf = [0u8; 8];
    mem.read_slice(&mut buf, GuestAddress(RESULT_ADDR)).unwrap();
    let result = u64::from_le_bytes(buf);

    assert_eq!(result, 0xDEADBEEFCAFEBABE, "Should successfully read from read-only page");
}

/// Test instruction fetch from non-present page
#[test]
fn test_pf_fetch_non_present() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Clear PT entry for page at 0x80000 where we'll try to execute
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();

    // Code: jump to 0x80000
    let code: &[u8] = &[
        // mov rax, 0x80000
        0x48, 0xc7, 0xc0, 0x00, 0x00, 0x08, 0x00,
        // jmp rax
        0xff, 0xe0,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (error_code, cr2, marker) = read_result(&mem);

    assert_eq!(marker, 0x14, "Should trigger #PF");
    assert_eq!(cr2, 0x80000, "CR2 should contain faulting address");
    assert_eq!(error_code & pf_error::P, 0, "P bit should be 0 (non-present)");
    assert_eq!(error_code & pf_error::ID, pf_error::ID, "I/D bit should be 1 (instruction fetch)");
}

/// Test page fault with various CR2 values (address within page)
#[test]
fn test_pf_cr2_offset_within_page() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Clear PT entry for page at 0x80000
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();

    // Code: read from 0x80123 (offset 0x123 within page)
    let code: &[u8] = &[
        // mov rax, [0x80123]
        0x48, 0x8b, 0x04, 0x25, 0x23, 0x01, 0x08, 0x00,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (_error_code, cr2, marker) = read_result(&mem);

    assert_eq!(marker, 0x14, "Should trigger #PF");
    assert_eq!(cr2, 0x80123, "CR2 should contain exact faulting address including offset");
}

/// Test page fault at PML4 level (address in non-canonical region would be #GP,
/// but we test missing PML4 entry)
#[test]
fn test_pf_missing_pml4_entry() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // PML4 entry 1 is not mapped (we only set entry 0)
    // Address using PML4 index 1: 0x8000000000 (512GB)
    // But this is too high for our 1MB memory, so we'll use a different approach:
    // Clear PML4 entry 0 to make all low addresses fault at PML4 level
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PML4_ADDR)).unwrap();

    // We need code in a location that IS mapped (put code at physical address
    // that page tables can still reach temporarily)
    // Actually this is tricky - if PML4[0] is gone, we can't fetch code either.
    // Let's instead just test with a higher virtual address.

    // Re-enable PML4[0] so code can run
    let pml4e = PDPT_ADDR | pte_flags::PRESENT | pte_flags::WRITABLE | pte_flags::USER;
    mem.write_slice(&pml4e.to_le_bytes(), GuestAddress(PML4_ADDR)).unwrap();

    // But clear PDPT[1] - addresses in range 0x40000000-0x7FFFFFFF will fault
    // Actually PDPT[0] maps 0-1GB, PDPT[1] would be 1-2GB
    // Let's just test a simpler case: clear PD[1]

    // PD[0] maps 0-2MB, PD[1] would map 2-4MB
    // Clear PD[1] - but we only have 1MB of memory anyway

    // Simplest test: access address 0x200000 (2MB), which needs PD[1]
    // But we didn't set up PD[1]

    // Code: read from 0x200000
    let code: &[u8] = &[
        // mov rax, [0x200000]
        0x48, 0x8b, 0x04, 0x25, 0x00, 0x00, 0x20, 0x00,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (error_code, cr2, marker) = read_result(&mem);

    assert_eq!(marker, 0x14, "Should trigger #PF");
    assert_eq!(cr2, 0x200000, "CR2 should contain faulting address");
    assert_eq!(error_code & pf_error::P, 0, "P bit should be 0 (non-present at PD level)");
}

/// Test that successfully mapped pages work correctly
#[test]
fn test_mapped_page_access_succeeds() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Write test value to a mapped address
    mem.write_slice(&0x123456789ABCDEFu64.to_le_bytes(), GuestAddress(0x50000)).unwrap();

    // Code: read from 0x50000, store in result, hlt
    let code: &[u8] = &[
        // mov rax, [0x50000]
        0x48, 0x8b, 0x04, 0x25, 0x00, 0x00, 0x05, 0x00,
        // mov [RESULT_ADDR], rax
        0x48, 0xa3,
        (RESULT_ADDR & 0xFF) as u8,
        ((RESULT_ADDR >> 8) & 0xFF) as u8,
        ((RESULT_ADDR >> 16) & 0xFF) as u8,
        ((RESULT_ADDR >> 24) & 0xFF) as u8,
        ((RESULT_ADDR >> 32) & 0xFF) as u8,
        ((RESULT_ADDR >> 40) & 0xFF) as u8,
        ((RESULT_ADDR >> 48) & 0xFF) as u8,
        ((RESULT_ADDR >> 56) & 0xFF) as u8,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let mut buf = [0u8; 8];
    mem.read_slice(&mut buf, GuestAddress(RESULT_ADDR)).unwrap();
    let result = u64::from_le_bytes(buf);

    assert_eq!(result, 0x123456789ABCDEF, "Should read correct value from mapped page");
}

/// Test write and read back through page tables
#[test]
fn test_write_read_through_paging() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Code: write value to 0x50000, read it back, store in result
    let code: &[u8] = &[
        // mov rax, 0xCAFEBABEDEADBEEF
        0x48, 0xb8, 0xEF, 0xBE, 0xAD, 0xDE, 0xBE, 0xBA, 0xFE, 0xCA,
        // mov [0x50000], rax
        0x48, 0xa3, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 0x00, 0x00,
        // mov rbx, [0x50000]
        0x48, 0x8b, 0x1c, 0x25, 0x00, 0x00, 0x05, 0x00,
        // mov [RESULT_ADDR], rbx
        0x48, 0x89, 0x1c, 0x25,
        (RESULT_ADDR & 0xFF) as u8,
        ((RESULT_ADDR >> 8) & 0xFF) as u8,
        ((RESULT_ADDR >> 16) & 0xFF) as u8,
        ((RESULT_ADDR >> 24) & 0xFF) as u8,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let mut buf = [0u8; 8];
    mem.read_slice(&mut buf, GuestAddress(RESULT_ADDR)).unwrap();
    let result = u64::from_le_bytes(buf);

    assert_eq!(result, 0xCAFEBABEDEADBEEF, "Write then read should return same value");
}

/// Test 2MB huge page mapping
#[test]
fn test_2mb_huge_page() {
    // Need more memory for 2MB pages - use 4MB
    let mem_size = 4 * 1024 * 1024;
    let regions = vec![(GuestAddress(0), mem_size)];
    let mem = Arc::new(GuestMemoryMmap::<()>::from_ranges(&regions).unwrap());

    // Set up page tables
    setup_page_tables(&mem);
    setup_idt(&mem);
    setup_handlers(&mem);

    let mut vcpu = create_paged_vcpu(mem.clone());

    // Modify PD entry 0 to be a 2MB huge page instead of pointing to PT
    // 2MB page at physical 0 (identity mapped)
    let pde = 0u64 | pte_flags::PRESENT | pte_flags::WRITABLE | pte_flags::USER | pte_flags::HUGE_PAGE;
    mem.write_slice(&pde.to_le_bytes(), GuestAddress(PD_ADDR)).unwrap();

    // Write test value somewhere in the 2MB range (use 0x180000 = 1.5MB)
    mem.write_slice(&0xDEADC0DEu64.to_le_bytes(), GuestAddress(0x180000)).unwrap();

    // Code: read from 0x180000 (1.5MB offset, within first 2MB page)
    let code: &[u8] = &[
        // mov rax, [0x180000]
        0x48, 0x8b, 0x04, 0x25, 0x00, 0x00, 0x18, 0x00,
        // mov [RESULT_ADDR], rax
        0x48, 0xa3,
        (RESULT_ADDR & 0xFF) as u8,
        ((RESULT_ADDR >> 8) & 0xFF) as u8,
        ((RESULT_ADDR >> 16) & 0xFF) as u8,
        ((RESULT_ADDR >> 24) & 0xFF) as u8,
        ((RESULT_ADDR >> 32) & 0xFF) as u8,
        ((RESULT_ADDR >> 40) & 0xFF) as u8,
        ((RESULT_ADDR >> 48) & 0xFF) as u8,
        ((RESULT_ADDR >> 56) & 0xFF) as u8,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let mut buf = [0u8; 8];
    mem.read_slice(&mut buf, GuestAddress(RESULT_ADDR)).unwrap();
    let result = u64::from_le_bytes(buf);

    assert_eq!(result, 0xDEADC0DE, "Should read through 2MB huge page");
}

/// Test page-crossing access (access spanning two pages)
#[test]
fn test_page_crossing_access() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Write 8 bytes across page boundary at 0x50FFC (last 4 bytes of page 0x50000,
    // first 4 bytes of page 0x51000)
    let value: u64 = 0x0102030405060708;
    mem.write_slice(&value.to_le_bytes(), GuestAddress(0x50FFC)).unwrap();

    // Code: read 8 bytes from 0x50FFC
    let code: &[u8] = &[
        // mov rax, [0x50FFC]
        0x48, 0x8b, 0x04, 0x25, 0xFC, 0x0F, 0x05, 0x00,
        // mov [RESULT_ADDR], rax
        0x48, 0xa3,
        (RESULT_ADDR & 0xFF) as u8,
        ((RESULT_ADDR >> 8) & 0xFF) as u8,
        ((RESULT_ADDR >> 16) & 0xFF) as u8,
        ((RESULT_ADDR >> 24) & 0xFF) as u8,
        ((RESULT_ADDR >> 32) & 0xFF) as u8,
        ((RESULT_ADDR >> 40) & 0xFF) as u8,
        ((RESULT_ADDR >> 48) & 0xFF) as u8,
        ((RESULT_ADDR >> 56) & 0xFF) as u8,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let mut buf = [0u8; 8];
    mem.read_slice(&mut buf, GuestAddress(RESULT_ADDR)).unwrap();
    let result = u64::from_le_bytes(buf);

    assert_eq!(result, value, "Should correctly handle page-crossing read");
}

/// Test page fault on second page of page-crossing access
#[test]
fn test_page_crossing_fault_on_second_page() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Clear PT entry for page 0x81000 (index 129)
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 129 * 8)).unwrap();

    // Code: read 8 bytes from 0x80FFC (crosses into unmapped 0x81000)
    let code: &[u8] = &[
        // mov rax, [0x80FFC]
        0x48, 0x8b, 0x04, 0x25, 0xFC, 0x0F, 0x08, 0x00,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (error_code, cr2, marker) = read_result(&mem);

    assert_eq!(marker, 0x14, "Should trigger #PF");
    // CR2 should be the address in the second page (0x81000)
    assert!(cr2 >= 0x81000 && cr2 < 0x82000,
        "CR2 ({:#x}) should be in second page (0x81000)", cr2);
    assert_eq!(error_code & pf_error::P, 0, "P bit should be 0 (non-present)");
}

/// Test TLB invalidation with INVLPG
#[test]
fn test_invlpg() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Initially page 0x80000 is mapped
    // Code:
    // 1. Read from 0x80000 (fills TLB)
    // 2. Unmap it by clearing PTE (via write to PTE address)
    // 3. INVLPG 0x80000
    // 4. Try to read again (should fault)

    // Write initial value
    mem.write_slice(&0xAABBCCDDu64.to_le_bytes(), GuestAddress(0x80000)).unwrap();

    let pt_entry_addr = PT_ADDR + 128 * 8;  // PTE for 0x80000

    // Code
    let code: &[u8] = &[
        // mov rax, [0x80000]  ; First read - populates TLB
        0x48, 0x8b, 0x04, 0x25, 0x00, 0x00, 0x08, 0x00,
        // xor rbx, rbx        ; rbx = 0
        0x48, 0x31, 0xdb,
        // mov [pt_entry_addr], rbx  ; Clear PTE
        0x48, 0x89, 0x1c, 0x25,
        (pt_entry_addr & 0xFF) as u8,
        ((pt_entry_addr >> 8) & 0xFF) as u8,
        ((pt_entry_addr >> 16) & 0xFF) as u8,
        ((pt_entry_addr >> 24) & 0xFF) as u8,
        // invlpg [0x80000]    ; Invalidate TLB entry
        0x0f, 0x01, 0x3c, 0x25, 0x00, 0x00, 0x08, 0x00,
        // mov rax, [0x80000]  ; Second read - should fault
        0x48, 0x8b, 0x04, 0x25, 0x00, 0x00, 0x08, 0x00,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (_error_code, cr2, marker) = read_result(&mem);

    assert_eq!(marker, 0x14, "Should trigger #PF after INVLPG");
    assert_eq!(cr2, 0x80000, "CR2 should be the invalidated address");
}

/// Test CR3 write flushes TLB
#[test]
fn test_cr3_flush() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    let pt_entry_addr = PT_ADDR + 128 * 8;

    // Code:
    // 1. Read from 0x80000 (fills TLB)
    // 2. Unmap it
    // 3. Write CR3 (same value) - should flush TLB
    // 4. Try to read again (should fault)
    let code: &[u8] = &[
        // mov rax, [0x80000]  ; First read
        0x48, 0x8b, 0x04, 0x25, 0x00, 0x00, 0x08, 0x00,
        // xor rbx, rbx
        0x48, 0x31, 0xdb,
        // mov [pt_entry_addr], rbx  ; Clear PTE
        0x48, 0x89, 0x1c, 0x25,
        (pt_entry_addr & 0xFF) as u8,
        ((pt_entry_addr >> 8) & 0xFF) as u8,
        ((pt_entry_addr >> 16) & 0xFF) as u8,
        ((pt_entry_addr >> 24) & 0xFF) as u8,
        // mov rax, cr3        ; Read CR3
        0x0f, 0x20, 0xd8,
        // mov cr3, rax        ; Write CR3 (flush TLB)
        0x0f, 0x22, 0xd8,
        // mov rax, [0x80000]  ; Second read - should fault
        0x48, 0x8b, 0x04, 0x25, 0x00, 0x00, 0x08, 0x00,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (_error_code, cr2, marker) = read_result(&mem);

    assert_eq!(marker, 0x14, "Should trigger #PF after CR3 write");
    assert_eq!(cr2, 0x80000, "CR2 should be correct");
}

// ============================================================================
// EDGE CASES AND TORTURE TESTS
// ============================================================================

/// Test page fault at exact page boundary (address 0xXXXXX000)
#[test]
fn test_pf_exact_page_boundary() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Unmap page at 0x80000
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();

    // Access exactly at page boundary 0x80000
    let code: &[u8] = &[
        // mov al, [0x80000]  ; Single byte read at exact boundary
        0xa0, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (_error_code, cr2, marker) = read_result(&mem);
    assert_eq!(marker, 0x14);
    assert_eq!(cr2, 0x80000, "CR2 should be exact page boundary");
}

/// Test page fault at last byte of page (address 0xXXXXXFFF)
#[test]
fn test_pf_last_byte_of_page() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Unmap page at 0x80000
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();

    // Access last byte of unmapped page (0x80FFF)
    let code: &[u8] = &[
        // mov al, [0x80FFF]
        0xa0, 0xFF, 0x0F, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (_error_code, cr2, marker) = read_result(&mem);
    assert_eq!(marker, 0x14);
    assert_eq!(cr2, 0x80FFF, "CR2 should be last byte of page");
}

/// Test unaligned 8-byte access crossing into unmapped page
#[test]
fn test_pf_unaligned_qword_crossing() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Unmap page at 0x81000 (page after 0x80000)
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 129 * 8)).unwrap();

    // 8-byte read starting at 0x80FFC crosses into unmapped 0x81000
    let code: &[u8] = &[
        // mov rax, [0x80FFC]  ; Unaligned qword crossing page boundary
        0x48, 0x8b, 0x04, 0x25, 0xFC, 0x0F, 0x08, 0x00,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (_error_code, cr2, marker) = read_result(&mem);
    assert_eq!(marker, 0x14);
    // CR2 should be in the unmapped page (0x81000-0x81003)
    assert!(cr2 >= 0x81000 && cr2 < 0x82000, "CR2 ({:#x}) should be in unmapped page", cr2);
}

/// Test PUSH causing page fault (stack operation)
/// Note: This test requires IST (Interrupt Stack Table) to work properly because
/// when an exception occurs with RSP pointing to an unmapped page, the CPU tries
/// to push the exception frame using that same RSP, causing a double/triple fault.
/// Skipped until IST support is added to the test harness.
#[test]
#[ignore = "Requires IST setup to test stack faults - CPU uses current RSP for exception frame"]
fn test_pf_push_to_unmapped_stack() {
    // This test would require:
    // 1. Set up a TSS with IST entries
    // 2. Configure IDT entries to use IST
    // 3. Then the CPU would use the IST stack instead of current RSP for exceptions
}

/// Test CALL causing page fault (return address push)
/// Note: Like PUSH, this requires IST because the exception frame is pushed using current RSP.
#[test]
#[ignore = "Requires IST setup to test stack faults - CPU uses current RSP for exception frame"]
fn test_pf_call_to_unmapped_stack() {
    // This test would require IST setup - see test_pf_push_to_unmapped_stack
}

/// Test RET causing page fault (pop return address from unmapped stack)
/// Note: Like PUSH, this requires IST because the exception frame is pushed using current RSP.
#[test]
#[ignore = "Requires IST setup to test stack faults - CPU uses current RSP for exception frame"]
fn test_pf_ret_from_unmapped_stack() {
    // This test would require IST setup - see test_pf_push_to_unmapped_stack
}

/// Test REP MOVSB crossing into unmapped page (source)
#[test]
fn test_pf_rep_movsb_source_unmapped() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Unmap page at 0x81000
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 129 * 8)).unwrap();

    // Code: REP MOVSB from 0x80FF0 (crosses into unmapped 0x81000)
    let code: &[u8] = &[
        // mov rsi, 0x80FF0  ; Source near page end
        0x48, 0xc7, 0xc6, 0xF0, 0x0F, 0x08, 0x00,
        // mov rdi, 0x50000  ; Destination (mapped)
        0x48, 0xc7, 0xc7, 0x00, 0x00, 0x05, 0x00,
        // mov rcx, 32       ; Copy 32 bytes (crosses boundary)
        0x48, 0xc7, 0xc1, 0x20, 0x00, 0x00, 0x00,
        // cld
        0xfc,
        // rep movsb
        0xf3, 0xa4,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (error_code, cr2, marker) = read_result(&mem);
    assert_eq!(marker, 0x14);
    assert!(cr2 >= 0x81000, "CR2 ({:#x}) should be in unmapped source page", cr2);
    assert_eq!(error_code & pf_error::WR, 0, "Source read should be read fault");
}

/// Test REP MOVSB crossing into unmapped page (destination)
#[test]
fn test_pf_rep_movsb_dest_unmapped() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Unmap page at 0x81000
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 129 * 8)).unwrap();

    // Code: REP MOVSB to 0x80FF0 (crosses into unmapped 0x81000)
    let code: &[u8] = &[
        // mov rsi, 0x50000  ; Source (mapped)
        0x48, 0xc7, 0xc6, 0x00, 0x00, 0x05, 0x00,
        // mov rdi, 0x80FF0  ; Destination near page end
        0x48, 0xc7, 0xc7, 0xF0, 0x0F, 0x08, 0x00,
        // mov rcx, 32
        0x48, 0xc7, 0xc1, 0x20, 0x00, 0x00, 0x00,
        // cld
        0xfc,
        // rep movsb
        0xf3, 0xa4,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (error_code, cr2, marker) = read_result(&mem);
    assert_eq!(marker, 0x14);
    assert!(cr2 >= 0x81000, "CR2 ({:#x}) should be in unmapped dest page", cr2);
    assert_eq!(error_code & pf_error::WR, pf_error::WR, "Destination write should be write fault");
}

/// Test multiple consecutive page faults handled correctly
#[test]
fn test_multiple_consecutive_page_faults() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Unmap pages 0x80000 and 0x81000
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 129 * 8)).unwrap();

    // Extended page fault handler that increments a counter and IRETs
    // (allowing multiple faults to be handled)
    let pf_handler_multi: &[u8] = &[
        // inc qword [RESULT_ADDR + 24]  ; Increment fault counter
        0x48, 0xff, 0x04, 0x25,
        ((RESULT_ADDR + 24) & 0xFF) as u8,
        (((RESULT_ADDR + 24) >> 8) & 0xFF) as u8,
        (((RESULT_ADDR + 24) >> 16) & 0xFF) as u8,
        (((RESULT_ADDR + 24) >> 24) & 0xFF) as u8,
        // mov rax, cr2
        0x0f, 0x20, 0xd0,
        // mov [RESULT_ADDR + 8], rax  ; Store last CR2
        0x48, 0xa3,
        ((RESULT_ADDR + 8) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 8) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 16) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 24) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 32) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 40) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 48) & 0xFF) as u8,
        (((RESULT_ADDR + 8) >> 56) & 0xFF) as u8,
        // add rsp, 8  ; Pop error code
        0x48, 0x83, 0xc4, 0x08,
        // Add 2 to return RIP to skip faulting instruction
        // mov rax, [rsp]
        0x48, 0x8b, 0x04, 0x24,
        // add rax, 8  ; Skip mov rax, [addr] (8 bytes)
        0x48, 0x83, 0xc0, 0x08,
        // mov [rsp], rax
        0x48, 0x89, 0x04, 0x24,
        // iretq
        0x48, 0xcf,
    ];
    mem.write_slice(pf_handler_multi, GuestAddress(PF_HANDLER_ADDR)).unwrap();

    // Initialize fault counter to 0
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(RESULT_ADDR + 24)).unwrap();

    // Code: trigger faults on both pages
    let code: &[u8] = &[
        // mov rax, [0x80000]  ; First fault (8 bytes)
        0x48, 0x8b, 0x04, 0x25, 0x00, 0x00, 0x08, 0x00,
        // mov rax, [0x81000]  ; Second fault (8 bytes)
        0x48, 0x8b, 0x04, 0x25, 0x00, 0x10, 0x08, 0x00,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    // Read fault counter
    let mut buf = [0u8; 8];
    mem.read_slice(&mut buf, GuestAddress(RESULT_ADDR + 24)).unwrap();
    let fault_count = u64::from_le_bytes(buf);

    assert_eq!(fault_count, 2, "Should have handled exactly 2 page faults");
}

/// Test write to read-only page at PD level (PD entry is read-only)
/// Note: We can't make PML4 read-only because exception handler also needs it
#[test]
fn test_pf_readonly_at_pd_level() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Make PD entry for page 0x80000 read-only (but keep PML4, PDPT writable for handler)
    // PD entry 0 covers 0-2MB, so we need a separate PD for test area
    // Instead, let's just make the specific PT entry read-only
    let pte = 0x90000u64 | pte_flags::PRESENT | pte_flags::USER;  // No WRITABLE
    let test_page = 144; // 0x90000 / 0x1000
    mem.write_slice(&pte.to_le_bytes(), GuestAddress(PT_ADDR + test_page * 8)).unwrap();

    // Try to write to 0x90000 (should fail at PT level due to no write permission)
    let code: &[u8] = &[
        // mov qword [0x90000], 0x12345678
        0x48, 0xc7, 0x04, 0x25, 0x00, 0x00, 0x09, 0x00, 0x78, 0x56, 0x34, 0x12,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (error_code, cr2, marker) = read_result(&mem);
    assert_eq!(marker, 0x14);
    assert_eq!(cr2, 0x90000);
    assert_eq!(error_code & pf_error::P, pf_error::P, "Should be protection violation");
    assert_eq!(error_code & pf_error::WR, pf_error::WR, "Should be write fault");
}

/// Test instruction fetch from read-only page (should succeed - NX not set)
#[test]
fn test_execute_readonly_succeeds() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Make page at 0x80000 present but read-only
    let pte = 0x80000u64 | pte_flags::PRESENT | pte_flags::USER;  // No WRITABLE
    mem.write_slice(&pte.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();

    // Put code at 0x80000 (read-only page)
    // Use movabs rax, imm64 (0x48 0xB8) to avoid sign-extension issues
    let test_value: u64 = 0x123456789ABCDEF0;
    let target_code: &[u8] = &[
        // movabs rax, 0x123456789ABCDEF0
        0x48, 0xb8,
        (test_value & 0xFF) as u8,
        ((test_value >> 8) & 0xFF) as u8,
        ((test_value >> 16) & 0xFF) as u8,
        ((test_value >> 24) & 0xFF) as u8,
        ((test_value >> 32) & 0xFF) as u8,
        ((test_value >> 40) & 0xFF) as u8,
        ((test_value >> 48) & 0xFF) as u8,
        ((test_value >> 56) & 0xFF) as u8,
        // mov [RESULT_ADDR], rax
        0x48, 0xa3,
        (RESULT_ADDR & 0xFF) as u8,
        ((RESULT_ADDR >> 8) & 0xFF) as u8,
        ((RESULT_ADDR >> 16) & 0xFF) as u8,
        ((RESULT_ADDR >> 24) & 0xFF) as u8,
        ((RESULT_ADDR >> 32) & 0xFF) as u8,
        ((RESULT_ADDR >> 40) & 0xFF) as u8,
        ((RESULT_ADDR >> 48) & 0xFF) as u8,
        ((RESULT_ADDR >> 56) & 0xFF) as u8,
        // hlt
        0xf4,
    ];
    mem.write_slice(target_code, GuestAddress(0x80000)).unwrap();

    // Start execution directly at 0x80000 (avoid jump complexity)
    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = 0x80000;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let mut buf = [0u8; 8];
    mem.read_slice(&mut buf, GuestAddress(RESULT_ADDR)).unwrap();
    let result = u64::from_le_bytes(buf);

    assert_eq!(result, test_value, "Should execute code on read-only page (NX not set)");
}

/// Test single-byte access sizes (8, 16, 32, 64 bits) all trigger correct faults
#[test]
fn test_pf_various_access_sizes() {
    let sizes_and_ops: &[(u8, &[u8], &str)] = &[
        // 8-bit read
        (1, &[0xa0, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00], "8-bit"),
        // 16-bit read: mov ax, [0x80000] with prefix
        (2, &[0x66, 0xa1, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00], "16-bit"),
        // 32-bit read: mov eax, [0x80000]
        (4, &[0xa1, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00], "32-bit"),
        // 64-bit read: mov rax, [0x80000]
        (8, &[0x48, 0x8b, 0x04, 0x25, 0x00, 0x00, 0x08, 0x00], "64-bit"),
    ];

    for (size, read_op, name) in sizes_and_ops {
        let mem = setup_paged_memory();
        let mut vcpu = create_paged_vcpu(mem.clone());

        // Unmap page at 0x80000
        mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();

        // Build code: read instruction + hlt
        let mut code = read_op.to_vec();
        code.push(0xf4); // hlt
        mem.write_slice(&code, GuestAddress(CODE_PADDR)).unwrap();

        let mut regs = vcpu.get_regs().unwrap();
        regs.rip = CODE_PADDR;
        vcpu.set_regs(&regs).unwrap();

        run_until_hlt(&mut vcpu).unwrap();

        let (_error_code, cr2, marker) = read_result(&mem);
        assert_eq!(marker, 0x14, "{} access should trigger #PF", name);
        assert_eq!(cr2, 0x80000, "{} access CR2 should be 0x80000", name);
    }
}

/// Test that read from first byte of mapped page after unmapped page succeeds
#[test]
fn test_access_first_byte_after_unmapped() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Unmap page at 0x80000 but keep 0x81000 mapped
    mem.write_slice(&0u64.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();

    // Write test value at 0x81000
    mem.write_slice(&0xCAFEBABEu64.to_le_bytes(), GuestAddress(0x81000)).unwrap();

    // Read from first byte of mapped page after unmapped
    let code: &[u8] = &[
        // mov rax, [0x81000]
        0x48, 0x8b, 0x04, 0x25, 0x00, 0x10, 0x08, 0x00,
        // mov [RESULT_ADDR], rax
        0x48, 0xa3,
        (RESULT_ADDR & 0xFF) as u8,
        ((RESULT_ADDR >> 8) & 0xFF) as u8,
        ((RESULT_ADDR >> 16) & 0xFF) as u8,
        ((RESULT_ADDR >> 24) & 0xFF) as u8,
        ((RESULT_ADDR >> 32) & 0xFF) as u8,
        ((RESULT_ADDR >> 40) & 0xFF) as u8,
        ((RESULT_ADDR >> 48) & 0xFF) as u8,
        ((RESULT_ADDR >> 56) & 0xFF) as u8,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let mut buf = [0u8; 8];
    mem.read_slice(&mut buf, GuestAddress(RESULT_ADDR)).unwrap();
    let result = u64::from_le_bytes(buf);

    assert_eq!(result, 0xCAFEBABE, "Should read from page after unmapped page");
}

/// Test XCHG instruction causing page fault (read-modify-write)
#[test]
fn test_pf_xchg_rmw() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Make page at 0x80000 read-only
    let pte = 0x80000u64 | pte_flags::PRESENT | pte_flags::USER;  // No WRITABLE
    mem.write_slice(&pte.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();

    // Write initial value
    mem.write_slice(&0x11111111u64.to_le_bytes(), GuestAddress(0x80000)).unwrap();

    // XCHG needs write access
    let code: &[u8] = &[
        // mov rax, 0x22222222
        0x48, 0xc7, 0xc0, 0x22, 0x22, 0x22, 0x22,
        // xchg [0x80000], rax  ; Atomic exchange - needs write
        0x48, 0x87, 0x04, 0x25, 0x00, 0x00, 0x08, 0x00,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (error_code, cr2, marker) = read_result(&mem);
    assert_eq!(marker, 0x14);
    assert_eq!(cr2, 0x80000);
    assert_eq!(error_code & pf_error::P, pf_error::P, "Should be protection violation");
    assert_eq!(error_code & pf_error::WR, pf_error::WR, "XCHG requires write access");
}

/// Test CMPXCHG instruction causing page fault
#[test]
fn test_pf_cmpxchg() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Make page at 0x80000 read-only
    let pte = 0x80000u64 | pte_flags::PRESENT | pte_flags::USER;
    mem.write_slice(&pte.to_le_bytes(), GuestAddress(PT_ADDR + 128 * 8)).unwrap();

    mem.write_slice(&0x11111111u64.to_le_bytes(), GuestAddress(0x80000)).unwrap();

    // CMPXCHG [mem], reg - always writes (either new value or old value)
    let code: &[u8] = &[
        // mov rax, 0x11111111  ; Expected value
        0x48, 0xc7, 0xc0, 0x11, 0x11, 0x11, 0x11,
        // mov rbx, 0x22222222  ; New value
        0x48, 0xc7, 0xc3, 0x22, 0x22, 0x22, 0x22,
        // lock cmpxchg [0x80000], rbx
        0xf0, 0x48, 0x0f, 0xb1, 0x1c, 0x25, 0x00, 0x00, 0x08, 0x00,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (error_code, _cr2, marker) = read_result(&mem);
    assert_eq!(marker, 0x14);
    assert_eq!(error_code & pf_error::WR, pf_error::WR, "CMPXCHG requires write access");
}

/// Test page fault with address that wraps around (canonical check)
/// In 64-bit mode, addresses must be canonical (sign-extended from bit 47)
///
/// According to x86-64 architecture, before the page table walk the CPU checks
/// if the virtual address is canonical:
/// - Bits 48-63 must all be copies of bit 47 (sign-extension)
/// - Address 0x0000800000000000 has bit 47=1 but bits 48-63=0, so it's non-canonical
/// - Non-canonical addresses should trigger #GP(0), not #PF
///
/// Currently ignored because the emulator doesn't implement canonical address checking.
#[test]
#[ignore = "Emulator doesn't implement canonical address checking - should trigger #GP, not #PF"]
fn test_noncanonical_address_gp() {
    let mem = setup_paged_memory();
    let mut vcpu = create_paged_vcpu(mem.clone());

    // Non-canonical address: 0x0000800000000000 (bit 47 is 1 but bits 48-63 are 0)
    // This should trigger #GP, not #PF
    let code: &[u8] = &[
        // mov rax, 0x0000800000000000
        0x48, 0xb8, 0x00, 0x00, 0x00, 0x00, 0x00, 0x80, 0x00, 0x00,
        // mov rbx, [rax]  ; Access non-canonical address
        0x48, 0x8b, 0x18,
        // hlt
        0xf4,
    ];
    mem.write_slice(code, GuestAddress(CODE_PADDR)).unwrap();

    let mut regs = vcpu.get_regs().unwrap();
    regs.rip = CODE_PADDR;
    vcpu.set_regs(&regs).unwrap();

    run_until_hlt(&mut vcpu).unwrap();

    let (_error_code, _cr2, marker) = read_result(&mem);
    // Should be #GP (13), not #PF (14)
    assert_eq!(marker, 0x0D, "Non-canonical address should trigger #GP, not #PF");
}
